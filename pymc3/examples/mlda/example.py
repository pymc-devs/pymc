import os
import numpy as np
import time
import pymc3 as pm
import theano.tensor as tt
import fenics as fn
from Model import Model
from itertools import product
import matplotlib.pyplot as plt
os.environ['OPENBLAS_NUM_THREADS'] = '1'  # Set environmental variable


def model_wrapper(my_model, theta, datapoints):
    """
    This solves a model given theta and returns the solution
    on the specified datapoints. Argument my_model is an object
    of class Model defined in model.py.
    """
    my_model.solve(theta)
    return my_model.get_data(datapoints)


def my_loglik(my_model, theta, datapoints, data, sigma):
    """
    This returns the log-likelihood of my_model given theta,
    datapoints, the observed data and sigma. It uses the
    model_wrapper function to do a model solve.
    """
    output = model_wrapper(my_model, theta, datapoints)
    return - (0.5 / sigma ** 2) * np.sum((output - data) ** 2)


class LogLike(tt.Op):
    """
    Theano Op that wraps the log-likelihood computation, necessary to
    pass "black-box" fenics code into pymc3.
    See https://docs.pymc.io/notebooks/blackbox_external_likelihood.html
    and https://docs.pymc.io/Advanced_usage_of_Theano_in_PyMC3.html for
    explanation.
    """

    # Specify what type of object will be passed and returned to the Op when it is
    # called. In our case we will be passing it a vector of values (the parameters
    # that define our model and a model object) and returning a single "scalar"
    # value (the log-likelihood)
    itypes = [tt.dvector]  # expects a vector of parameter values when called
    otypes = [tt.dscalar]  # outputs a single scalar value (the log likelihood)

    def __init__(self, my_model, loglike, data, x, sigma):
        """
        Initialise the Op with various things that our log-likelihood function
        requires. Below are the things that are needed in this particular
        example.

        Parameters
        ----------
        my_model:
            A Model object (defined in model.py) that contains the parameters
            and functions of out model.
        loglike:
            The log-likelihood function we've defined, in this example it is
            my_loglik.
        data:
            The "observed" data that our log-likelihood function takes in. These
            are the true data generated by the finest model in this example.
        x:
            The dependent variable (aka 'x') that our model requires. This is
            the datapoints in this example.
        sigma:
            The noise standard deviation that our function requires.
        """

        # add inputs as class attributes
        self.my_model = my_model
        self.likelihood = loglike
        self.data = data
        self.x = x
        self.sigma = sigma

    def perform(self, node, inputs, outputs):
        # the method that is used when calling the Op
        theta = inputs  # this will contain my variables

        # call the log-likelihood function
        logl = self.likelihood(self.my_model, theta, self.x, self.data, self.sigma)

        outputs[0][0] = np.array(logl) # output the log-likelihood


def project_eigenpairs(model_fine, model_coarse):
    model_coarse.random_process.eigenvalues[:] = model_fine.random_process.eigenvalues
    for i in range(model_coarse.mkl):
        psi_fine = fn.Function(model_fine.solver.V)
        psi_fine.vector()[:] = model_fine.random_process.eigenvectors[:, i]
        psi_coarse = fn.project(psi_fine, model_coarse.solver.V)
        model_coarse.random_process.eigenvectors[:, i] = psi_coarse.vector()[:]


def main():
    # PART 1: PARAMETERS
    # Set the resolution of the multi-level models (from coarsest to finest)
    # and the random field parameters.
    resolutions = [(60, 60), (120, 120)]
    field_mean = 0
    field_stdev = 1
    lamb_cov = 0.1
    # Set the number of unknown parameters
    mkl = 2
    # Number of draws from the distribution
    ndraws = 200
    # Number of "burn-in points" (which we'll discard)
    nburn = False
    tune_interval = 10000  # big to prevent tuning
    # Number of independent chains
    nchains = 2
    # Subsampling rate for MLDA
    nsub = 2
    # Set the sigma for inference
    sigma = 0.01
    # Data generation seed
    data_seed = 12345
    # Sampling seed
    sampling_seed = 12345
    # Datapoints list
    points_list = [0.1, 0.3, 0.5, 0.7, 0.9]

    # PART 2: GENERATE MODELS AND DATA
    # Note this can take minutes for large resolutions
    # Initialise model objects for all levels
    my_models = []
    for r in resolutions:
        my_models.append(Model(r, field_mean, field_stdev, mkl, lamb_cov))

    # Project eignevactors from fine model to all coarse models
    for i in range(len(my_models[:-1])):
        project_eigenpairs(my_models[-1], my_models[i])

    # Solve finest model and plot transmissivity field and solution
    np.random.seed(data_seed)
    my_models[-1].solve()
    my_models[-1].plot(lognormal=False)

    # Save true parameters of finest model
    true_parameters = my_models[-1].random_process.parameters

    # Define the sampling points.
    x_data = y_data = np.array(points_list)
    datapoints = np.array(list(product(x_data, y_data)))

    # Get data from the sampling points and perturb it with some noise.
    noise = np.random.normal(0, 0.001, len(datapoints))

    # Generate data from the finest model for use in pymc3 inference
    data = model_wrapper(my_models[-1], true_parameters, datapoints) + noise

    # Test the log-likelihood function with the finest model
    my_loglik(my_models[-1], true_parameters, datapoints, data, sigma)

    # create Theano Ops to wrap likelihoods of all model levels and store them in list
    logl = []
    for m in my_models:
        logl.append(LogLike(m, my_loglik, data, datapoints, sigma))


    # PART 3: INFERENCE IN PYMC3
    # Set up models in PyMC3 for each level - excluding finest model level
    coarse_models = []
    for j in range(len(my_models) - 1):
        with pm.Model() as model:
            # uniform priors on parameters
            parameters = []
            for i in range(mkl):
                parameters.append(pm.Uniform('theta_' + str(i), lower=-3., upper=3.))

            # convert m and c to a tensor vector
            theta = tt.as_tensor_variable(parameters)

            # use a DensityDist (use a lamdba function to "call" the Op)
            pm.DensityDist('likelihood', lambda v: logl[j](v), observed={'v': theta})

        coarse_models.append(model)

    # Set up finest model and perform inference with PyMC3, using the MLDA algorithm
    # and passing the coarse_models list created above.
    method_names = []
    traces = []
    runtimes = []
    acc = []
    ess = []
    ess_n = []
    performances = []

    with pm.Model():
        # uniform priors on parameters
        parameters = []
        for i in range(mkl):
            parameters.append(pm.Uniform('theta_' + str(i), lower=-3., upper=3.))

        # convert m and c to a tensor vector
        theta = tt.as_tensor_variable(parameters)

        # use a DensityDist (use a lamdba function to "call" the Op)
        pm.DensityDist('likelihood', lambda v: logl[-1](v), observed={'v': theta})

        # Initialise an MLDA step method object, passing the subsampling rate and
        # coarse models list
        # Also initialise a Metropolis step method object
        step_metropolis = pm.Metropolis(tune_interval=tune_interval)
        step_mlda = pm.MLDA(subsampling_rate=nsub, coarse_models=coarse_models)

        # inference
        # Metropolis
        t_start = time.time()
        method_names.append("Metropolis")
        traces.append(pm.sample(draws=ndraws, step=step_metropolis,
                                chains=nchains, tune=nburn,
                                random_seed=sampling_seed))
        runtimes.append(time.time() - t_start)

        # MLDA
        t_start = time.time()
        method_names.append("MLDA")
        traces.append(pm.sample(draws=ndraws, step=step_mlda,
                                chains=nchains, tune=nburn,
                                random_seed=sampling_seed))
        runtimes.append(time.time() - t_start)

        for i, trace in enumerate(traces):
            acc.append(trace.get_sampler_stats('accepted').mean())
            ess.append(np.array(pm.ess(trace).to_array()))
            ess_n.append(ess[i] / len(trace) / trace.nchains)
            performances.append(ess[i] / runtimes[i])
            print(f'\nSampler {method_names[i]}: {len(trace)} samples across '
                  f'{trace.nchains} chains.'
                  f'\nRuntime: {runtimes[i]} seconds'
                  f'\nAcceptance rate: {acc[i]}'
                  f'\nESS list: {ess[i]}'
                  f'\nNormalised ESS list: {ess_n[i]}'
                  f'\nESS/sec: {performances[i]}')

        # print true theta values and pymc3 sampling summary
        print(f"\nDetailed summaries and plots:\nTrue parameters: {true_parameters}")
        for i, trace in enumerate(traces):
            print(f"Sampler {method_names[i]}:\n", pm.stats.summary(trace))
            pm.plots.traceplot(trace)

        plt.show()


    '''
    # PART 4: VALIDATION
    # Evaluate MCMC sample mean for each unknown parameter (theta)
    samples_mean = []
    for i in range(mkl):
        samples_mean.append(trace['theta_'+str(i)].mean())
    
    # Solve the finest model using the inferred theta means
    # and plot transmissivity field and solution
    my_models[-1].solve(true_parameters)
    my_models[-1].plot(transform_field=True)
    my_models[-1].solve(samples_mean)
    my_models[-1].plot(transform_field=True)
    '''


if __name__ == '__main__':
    main()
