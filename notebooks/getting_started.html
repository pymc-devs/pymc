

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Getting started with PyMC3 &mdash; PyMC3 3.0.rc1 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="PyMC3 3.0.rc1 documentation" href="../index.html"/>
        <link rel="up" title="Getting started" href="../getting_started.html"/>
        <link rel="next" title="Examples" href="../examples.html"/>
        <link rel="prev" title="Getting started" href="../getting_started.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> PyMC3
          

          
          </a>

          
            
            
              <div class="version">
                3.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../getting_started.html">Getting started</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Getting started with PyMC3</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Installation">Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#A-Motivating-Example:-Linear-Regression">A Motivating Example: Linear Regression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Generating-data">Generating data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Model-Specification">Model Specification</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Model-fitting">Model fitting</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Posterior-analysis">Posterior analysis</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Case-study-1:-Stochastic-volatility">Case study 1: Stochastic volatility</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#The-Model">The Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#The-Data">The Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Model-Specification">Model Specification</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Fitting">Fitting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Case-study-2:-Coal-mining-disasters">Case study 2: Coal mining disasters</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Arbitrary-deterministics">Arbitrary deterministics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Arbitrary-distributions">Arbitrary distributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Generalized-Linear-Models">Generalized Linear Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Backends">Backends</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Discussion">Discussion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#References">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">PyMC3</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
          <li><a href="../getting_started.html">Getting started</a> &raquo;</li>
      
    <li>Getting started with PyMC3</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/notebooks/getting_started.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput > div,
div.nbinput div[class^=highlight],
div.nbinput div[class^=highlight] pre,
div.nboutput,
div.nboutput > div,
div.nboutput div[class^=highlight],
div.nboutput div[class^=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class^=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput > :first-child pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput > :first-child pre {
    color: #D84315;
}

/* all prompts */
div.nbinput > :first-child[class^=highlight],
div.nboutput > :first-child[class^=highlight],
div.nboutput > :first-child {
    min-width: 11ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}

/* input/output area */
div.nbinput > :nth-child(2)[class^=highlight],
div.nboutput > :nth-child(2),
div.nboutput > :nth-child(2)[class^=highlight] {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}

/* input area */
div.nbinput > :nth-child(2)[class^=highlight] {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput  > :nth-child(2).stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="Getting-started-with-PyMC3">
<h1>Getting started with PyMC3<a class="headerlink" href="#Getting-started-with-PyMC3" title="Permalink to this headline">¶</a></h1>
<p>Authors: John Salvatier, Thomas V. Wiecki, Christopher Fonnesbeck</p>
<p>Note: This text is taken from the <a class="reference external" href="https://peerj.com/articles/cs-55/">PeerJ CS publication on
PyMC3</a>.</p>
<div class="section" id="Abstract">
<h2>Abstract<a class="headerlink" href="#Abstract" title="Permalink to this headline">¶</a></h2>
<p>Probabilistic Programming allows for automatic Bayesian inference on
user-defined probabilistic models. Recent advances in Markov chain Monte
Carlo (MCMC) sampling allow inference on increasingly complex models.
This class of MCMC, known as Hamliltonian Monte Carlo, requires gradient
information which is often not readily available. PyMC3 is a new open
source Probabilistic Programming framework written in Python that uses
Theano to compute gradients via automatic differentiation as well as
compile probabilistic programs on-the-fly to C for increased speed.
Contrary to other Probabilistic Programming languages, PyMC3 allows
model specification directly in Python code. The lack of a domain
specific language allows for great flexibility and direct interaction
with the model. This paper is a tutorial-style introduction to this
software package.</p>
</div>
<div class="section" id="Introduction">
<h2>Introduction<a class="headerlink" href="#Introduction" title="Permalink to this headline">¶</a></h2>
<p>Probabilistic programming (PP) allows flexible specification of Bayesian
statistical models in code. PyMC3 is a new, open-source PP framework
with an intuitive and readable, yet powerful, syntax that is close to
the natural syntax statisticians use to describe models. It features
next-generation Markov chain Monte Carlo (MCMC) sampling algorithms such
as the No-U-Turn Sampler (NUTS; Hoffman, 2014), a self-tuning variant of
Hamiltonian Monte Carlo (HMC; Duane, 1987). This class of samplers works
well on high dimensional and complex posterior distributions and allows
many complex models to be fit without specialized knowledge about
fitting algorithms. HMC and NUTS take advantage of gradient information
from the likelihood to achieve much faster convergence than traditional
sampling methods, especially for larger models. NUTS also has several
self-tuning strategies for adaptively setting the tunable parameters of
Hamiltonian Monte Carlo, which means you usually don&#8217;t need to have
specialized knowledge about how the algorithms work. PyMC3, Stan (Stan
Development Team, 2014), and the LaplacesDemon package for R are
currently the only PP packages to offer HMC.</p>
<p>Probabilistic programming in Python confers a number of advantages
including multi-platform compatibility, an expressive yet clean and
readable syntax, easy integration with other scientific libraries, and
extensibility via C, C++, Fortran or Cython. These features make it
relatively straightforward to write and use custom statistical
distributions, samplers and transformation functions, as required by
Bayesian analysis.</p>
<p>While most of PyMC3&#8217;s user-facing features are written in pure Python,
it leverages Theano (Bergstra et al., 2010) to transparently transcode
models to C and compile them to machine code, thereby boosting
performance. Theano is a library that allows expressions to be defined
using generalized vector data structures called <em>tensors</em>, which are
tightly integrated with the popular NumPy <code class="docutils literal"><span class="pre">ndarray</span></code> data structure,
and similarly allow for broadcasting and advanced indexing, just as
NumPy arrays do. Theano also automatically optimizes the likelihood&#8217;s
computational graph for speed and provides simple GPU integration.</p>
<p>Here, we present a primer on the use of PyMC3 for solving general
Bayesian statistical inference and prediction problems. We will first
see the basics of how to use PyMC3, motivated by a simple example:
installation, data creation, model definition, model fitting and
posterior analysis. Then we will cover two case studies and use them to
show how to define and fit more sophisticated models. Finally we will
show how to extend PyMC3 and discuss other useful features: the
Generalized Linear Models subpackage, custom distributions, custom
transformations and alternative storage backends.</p>
</div>
<div class="section" id="Installation">
<h2>Installation<a class="headerlink" href="#Installation" title="Permalink to this headline">¶</a></h2>
<p>Running PyMC3 requires a working Python interpreter, either version 2.7
(or more recent) or 3.4 (or more recent); we recommend that new users
install version 3.4. A complete Python installation for Mac OSX, Linux
and Windows can most easily be obtained by downloading and installing
the free
<code class="docutils literal"><span class="pre">`Anaconda</span> <span class="pre">Python</span> <span class="pre">Distribution</span></code> &lt;<a class="reference external" href="https://store.continuum.io/cshop/anaconda/">https://store.continuum.io/cshop/anaconda/</a>&gt;`__
by ContinuumIO.</p>
<p><code class="docutils literal"><span class="pre">PyMC3</span></code> can be installed using <code class="docutils literal"><span class="pre">pip</span></code>
(<a class="reference external" href="https://pip.pypa.io/en/latest/installing.html">https://pip.pypa.io/en/latest/installing.html</a>):</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">pymc</span><span class="o">-</span><span class="n">devs</span><span class="o">/</span><span class="n">pymc3</span>
</pre></div>
</div>
<p>PyMC3 depends on several third-party Python packages which will be
automatically installed when installing via pip. The four required
dependencies are: <code class="docutils literal"><span class="pre">Theano</span></code>, <code class="docutils literal"><span class="pre">NumPy</span></code>, <code class="docutils literal"><span class="pre">SciPy</span></code>, and <code class="docutils literal"><span class="pre">Matplotlib</span></code>.</p>
<p>To take full advantage of PyMC3, the optional dependencies <code class="docutils literal"><span class="pre">Pandas</span></code>
and <code class="docutils literal"><span class="pre">Patsy</span></code> should also be installed. These are <em>not</em> automatically
installed, but can be installed by:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">patsy</span> <span class="n">pandas</span>
</pre></div>
</div>
<p>The source code for PyMC3 is hosted on GitHub at
<a class="reference external" href="https://github.com/pymc-devs/pymc3">https://github.com/pymc-devs/pymc3</a> and is distributed under the liberal
<a class="reference external" href="https://github.com/pymc-devs/pymc3/blob/master/LICENSE">Apache License
2.0</a>. On the
GitHub site, users may also report bugs and other issues, as well as
contribute code to the project, which we actively encourage.</p>
</div>
<div class="section" id="A-Motivating-Example:-Linear-Regression">
<h2>A Motivating Example: Linear Regression<a class="headerlink" href="#A-Motivating-Example:-Linear-Regression" title="Permalink to this headline">¶</a></h2>
<p>To introduce model definition, fitting and posterior analysis, we first
consider a simple Bayesian linear regression model with normal priors
for the parameters. We are interested in predicting outcomes <span class="math">\(Y\)</span>
as normally-distributed observations with an expected value <span class="math">\(\mu\)</span>
that is a linear function of two predictor variables, <span class="math">\(X_1\)</span> and
<span class="math">\(X_2\)</span>.</p>
<div class="math">
\begin{aligned}
Y  &amp;\sim \mathcal{N}(\mu, \sigma^2) \\
\mu &amp;= \alpha + \beta_1 X_1 + \beta_2 X_2
\end{aligned}</div><p>where <span class="math">\(\alpha\)</span> is the intercept, and <span class="math">\(\beta_i\)</span> is the
coefficient for covariate <span class="math">\(X_i\)</span>, while <span class="math">\(\sigma\)</span> represents
the observation error. Since we are constructing a Bayesian model, the
unknown variables in the model must be assigned a prior distribution. We
choose zero-mean normal priors with variance of 100 for both regression
coefficients, which corresponds to <em>weak</em> information regarding the true
parameter values. We choose a half-normal distribution (normal
distribution bounded at zero) as the prior for <span class="math">\(\sigma\)</span>.</p>
<div class="math">
\begin{aligned}
\alpha &amp;\sim \mathcal{N}(0, 100) \\
\beta_i &amp;\sim \mathcal{N}(0, 100) \\
\sigma &amp;\sim \lvert\mathcal{N}(0, 1){\rvert}
\end{aligned}</div><div class="section" id="Generating-data">
<h3>Generating data<a class="headerlink" href="#Generating-data" title="Permalink to this headline">¶</a></h3>
<p>We can simulate some artificial data from this model using only NumPy&#8217;s
<code class="docutils literal"><span class="pre">random</span></code> module, and then use PyMC3 to try to recover the
corresponding parameters. We are intentionally generating the data to
closely correspond the PyMC3 model structure.</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="c1"># Initialize random number generator</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># True parameter values</span>
<span class="n">alpha</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">beta</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]</span>

<span class="c1"># Size of dataset</span>
<span class="n">size</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Predictor variable</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.2</span>

<span class="c1"># Simulate outcome variable</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">X1</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">X2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="p">)</span><span class="o">*</span><span class="n">sigma</span>
</pre></div>
</div>
</div>
<p>Here is what the simulated data look like. We use the <code class="docutils literal"><span class="pre">pylab</span></code> module
from the plotting library matplotlib.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">);</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X1&#39;</span><span class="p">);</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X2&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="../_images/notebooks_getting_started_5_0.png" src="../_images/notebooks_getting_started_5_0.png" />
</div>
</div>
</div>
<div class="section" id="Model-Specification">
<h3>Model Specification<a class="headerlink" href="#Model-Specification" title="Permalink to this headline">¶</a></h3>
<p>Specifying this model in PyMC3 is straightforward because the syntax is
as close to the statistical notation. For the most part, each line of
Python code corresponds to a line in the model notation above.</p>
<p>First, we import the components we will need from PyMC.</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pymc3</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">Normal</span><span class="p">,</span> <span class="n">HalfNormal</span>
</pre></div>
</div>
</div>
<p>Now we build our model, which we will present in full first, then
explain each part line-by-line.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">basic_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>

<span class="k">with</span> <span class="n">basic_model</span><span class="p">:</span>

    <span class="c1"># Priors for unknown model parameters</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Expected value of outcome</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">X1</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">X2</span>

    <span class="c1"># Likelihood (sampling distribution) of observations</span>
    <span class="n">Y_obs</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;Y_obs&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
Applied log-transform to sigma and added transformed sigma_log_ to model.
</pre></div></div>
</div>
<p>The first line,</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">basic_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
</pre></div>
</div>
<p>creates a new <code class="docutils literal"><span class="pre">Model</span></code> object which is a container for the model random
variables.</p>
<p>Following instantiation of the model, the subsequent specification of
the model components is performed inside a <code class="docutils literal"><span class="pre">with</span></code> statement:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">basic_model</span><span class="p">:</span>
</pre></div>
</div>
<p>This creates a <em>context manager</em>, with our <code class="docutils literal"><span class="pre">basic_model</span></code> as the
context, that includes all statements until the indented block ends.
This means all PyMC3 objects introduced in the indented code block below
the <code class="docutils literal"><span class="pre">with</span></code> statement are added to the model behind the scenes. Absent
this context manager idiom, we would be forced to manually associate
each of the variables with <code class="docutils literal"><span class="pre">basic_model</span></code> right after we create them.
If you try to create a new random variable without a <code class="docutils literal"><span class="pre">with</span> <span class="pre">model:</span></code>
statement, it will raise an error since there is no obvious model for
the variable to be added to.</p>
<p>The first three statements in the context manager:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>create a <strong>stochastic</strong> random variables with a Normal prior
distributions for the regression coefficients with a mean of 0 and
standard deviation of 10 for the regression coefficients, and a
half-normal distribution for the standard deviation of the observations,
<span class="math">\(\sigma\)</span>. These are stochastic because their values are partly
determined by its parents in the dependency graph of random variables,
which for priors are simple constants, and partly random (or
stochastic).</p>
<p>We call the <code class="docutils literal"><span class="pre">Normal</span></code> constructor to create a random variable to use as
a normal prior. The first argument is always the <em>name</em> of the random
variable, which should almost always match the name of the Python
variable being assigned to, since it sometimes used to retrieve the
variable from the model for summarizing output. The remaining required
arguments for a stochastic object are the parameters, in this case
<code class="docutils literal"><span class="pre">mu</span></code>, the mean, and <code class="docutils literal"><span class="pre">sd</span></code>, the standard deviation, which we assign
hyperparameter values for the model. In general, a distribution&#8217;s
parameters are values that determine the location, shape or scale of the
random variable, depending on the parameterization of the distribution.
Most commonly used distributions, such as <code class="docutils literal"><span class="pre">Beta</span></code>, <code class="docutils literal"><span class="pre">Exponential</span></code>,
<code class="docutils literal"><span class="pre">Categorical</span></code>, <code class="docutils literal"><span class="pre">Gamma</span></code>, <code class="docutils literal"><span class="pre">Binomial</span></code> and many others, are available
in PyMC3.</p>
<p>The <code class="docutils literal"><span class="pre">beta</span></code> variable has an additional <code class="docutils literal"><span class="pre">shape</span></code> argument to denote it
as a vector-valued parameter of size 2. The <code class="docutils literal"><span class="pre">shape</span></code> argument is
available for all distributions and specifies the length or shape of the
random variable, but is optional for scalar variables, since it defaults
to a value of one. It can be an integer, to specify an array, or a
tuple, to specify a multidimensional array (<em>e.g.</em> <code class="docutils literal"><span class="pre">shape=(5,7)</span></code> makes
random variable that takes on 5 by 7 matrix values).</p>
<p>Detailed notes about distributions, sampling methods and other PyMC3
functions are available via the <code class="docutils literal"><span class="pre">help</span></code> function.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">help</span><span class="p">(</span><span class="n">Normal</span><span class="p">)</span> <span class="c1">#try help(Model), help(Uniform) or help(basic_model)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
Help on class Normal in module pymc3.distributions.continuous:

class Normal(pymc3.distributions.distribution.Continuous)
 |  Univariate normal log-likelihood.
 |
 |  .. math::
 |
 |     f(x \mid \mu, \tau) =
 |         \sqrt{\frac{\tau}{2\pi}}
 |         \exp\left\{ -\frac{\tau}{2} (x-\mu)^2 \right\}
 |
 |  ========  ==========================================
 |  Support   :math:`x \in \mathbb{R}`
 |  Mean      :math:`\mu`
 |  Variance  :math:`\dfrac{1}{\tau}` or :math:`\sigma^2`
 |  ========  ==========================================
 |
 |  Normal distribution can be parameterized either in terms of precision
 |  or standard deviation. The link between the two parametrizations is
 |  given by
 |
 |  .. math::
 |
 |     \tau = \dfrac{1}{\sigma^2}
 |
 |  Parameters
 |  ----------
 |  mu : float
 |      Mean.
 |  sd : float
 |      Standard deviation (sd &gt; 0).
 |  tau : float
 |      Precision (tau &gt; 0).
 |
 |  Method resolution order:
 |      Normal
 |      pymc3.distributions.distribution.Continuous
 |      pymc3.distributions.distribution.Distribution
 |      builtins.object
 |
 |  Methods defined here:
 |
 |  __init__(self, *args, **kwargs)
 |      Initialize self.  See help(type(self)) for accurate signature.
 |
 |  logp(self, value)
 |
 |  random(self, point=None, size=None, repeat=None)
 |
 |  ----------------------------------------------------------------------
 |  Methods inherited from pymc3.distributions.distribution.Distribution:
 |
 |  __getnewargs__(self)
 |
 |  default(self)
 |
 |  get_test_val(self, val, defaults)
 |
 |  getattr_value(self, val)
 |
 |  ----------------------------------------------------------------------
 |  Class methods inherited from pymc3.distributions.distribution.Distribution:
 |
 |  dist(*args, **kwargs) from builtins.type
 |
 |  ----------------------------------------------------------------------
 |  Static methods inherited from pymc3.distributions.distribution.Distribution:
 |
 |  __new__(cls, name, *args, **kwargs)
 |      Create and return a new object.  See help(type) for accurate signature.
 |
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from pymc3.distributions.distribution.Distribution:
 |
 |  __dict__
 |      dictionary for instance variables (if defined)
 |
 |  __weakref__
 |      list of weak references to the object (if defined)

</pre></div></div>
</div>
<p>Having defined the priors, the next statement creates the expected value
<code class="docutils literal"><span class="pre">mu</span></code> of the outcomes, specifying the linear relationship:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">X1</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">X2</span>
</pre></div>
</div>
<p>This creates a <strong>deterministic</strong> random variable, which implies that its
value is <em>completely</em> determined by its parents&#8217; values. That is, there
is no uncertainty beyond that which is inherent in the parents&#8217; values.
Here, <code class="docutils literal"><span class="pre">mu</span></code> is just the sum of the intercept <code class="docutils literal"><span class="pre">alpha</span></code> and the two
products of the coefficients in <code class="docutils literal"><span class="pre">beta</span></code> and the predictor variables,
whatever their values may be.</p>
<p>PyMC3 random variables and data can be arbitrarily added, subtracted,
divided, multiplied together and indexed-into to create new random
variables. This allows for great model expressivity. Many common
mathematical functions like <code class="docutils literal"><span class="pre">sum</span></code>, <code class="docutils literal"><span class="pre">sin</span></code>, <code class="docutils literal"><span class="pre">exp</span></code> and linear algebra
functions like <code class="docutils literal"><span class="pre">dot</span></code> (for inner product) and <code class="docutils literal"><span class="pre">inv</span></code> (for inverse) are
also provided.</p>
<p>The final line of the model, defines <code class="docutils literal"><span class="pre">Y_obs</span></code>, the sampling
distribution of the outcomes in the dataset.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">Y_obs</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;Y_obs&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p>This is a special case of a stochastic variable that we call an
<strong>observed stochastic</strong>, and represents the data likelihood of the
model. It is identical to a standard stochastic, except that its
<code class="docutils literal"><span class="pre">observed</span></code> argument, which passes the data to the variable, indicates
that the values for this variable were observed, and should not be
changed by any fitting algorithm applied to the model. The data can be
passed in the form of either a <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or <code class="docutils literal"><span class="pre">pandas.DataFrame</span></code>
object.</p>
<p>Notice that, unlike for the priors of the model, the parameters for the
normal distribution of <code class="docutils literal"><span class="pre">Y_obs</span></code> are not fixed values, but rather are
the deterministic object <code class="docutils literal"><span class="pre">mu</span></code> and the stochastic <code class="docutils literal"><span class="pre">sigma</span></code>. This
creates parent-child relationships between the likelihood and these two
variables.</p>
</div>
<div class="section" id="Model-fitting">
<h3>Model fitting<a class="headerlink" href="#Model-fitting" title="Permalink to this headline">¶</a></h3>
<p>Having completely specified our model, the next step is to obtain
posterior estimates for the unknown variables in the model. Ideally, we
could calculate the posterior estimates analytically, but for most
non-trivial models, this is not feasible. We will consider two
approaches, whose appropriateness depends on the structure of the model
and the goals of the analysis: finding the <em>maximum a posteriori</em> (MAP)
point using optimization methods, and computing summaries based on
samples drawn from the posterior distribution using Markov Chain Monte
Carlo (MCMC) sampling methods.</p>
<div class="section" id="Maximum-a-posteriori-methods">
<h4>Maximum a posteriori methods<a class="headerlink" href="#Maximum-a-posteriori-methods" title="Permalink to this headline">¶</a></h4>
<p>The <strong>maximum a posteriori (MAP)</strong> estimate for a model, is the mode of
the posterior distribution and is generally found using numerical
optimization methods. This is often fast and easy to do, but only gives
a point estimate for the parameters and can be biased if the mode isn&#8217;t
representative of the distribution. PyMC3 provides this functionality
with the <code class="docutils literal"><span class="pre">find_MAP</span></code> function.</p>
<p>Below we find the MAP for our original model. The MAP is returned as a
parameter <strong>point</strong>, which is always represented by a Python dictionary
of variable names to NumPy arrays of parameter values.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pymc3</span> <span class="kn">import</span> <span class="n">find_MAP</span>

<span class="n">map_estimate</span> <span class="o">=</span> <span class="n">find_MAP</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">basic_model</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">map_estimate</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
{&#39;beta&#39;: array([ 0.94848602,  2.60705514]), &#39;alpha&#39;: array(0.9065985497559482), &#39;sigma_log_&#39;: array(-0.03278147017403066)}
</pre></div></div>
</div>
<p>By default, <code class="docutils literal"><span class="pre">find_MAP</span></code> uses the Broyden–Fletcher–Goldfarb–Shanno
(BFGS) optimization algorithm to find the maximum of the log-posterior
but also allows selection of other optimization algorithms from the
<code class="docutils literal"><span class="pre">scipy.optimize</span></code> module. For example, below we use Powell&#8217;s method to
find the MAP.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">optimize</span>

<span class="n">map_estimate</span> <span class="o">=</span> <span class="n">find_MAP</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">basic_model</span><span class="p">,</span> <span class="n">fmin</span><span class="o">=</span><span class="n">optimize</span><span class="o">.</span><span class="n">fmin_powell</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">map_estimate</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
{&#39;beta&#39;: array([ 0.95140146,  2.61437458]), &#39;alpha&#39;: array(0.9090521898977764), &#39;sigma_log_&#39;: array(-0.030009775203258385)}
</pre></div></div>
</div>
<p>It is important to note that the MAP estimate is not always reasonable,
especially if the mode is at an extreme. This can be a subtle issue;
with high dimensional posteriors, one can have areas of extremely high
density but low total probability because the volume is very small. This
will often occur in hierarchical models with the variance parameter for
the random effect. If the individual group means are all the same, the
posterior will have near infinite density if the scale parameter for the
group means is almost zero, even though the probability of such a small
scale parameter will be small since the group means must be extremely
close together.</p>
<p>Most techniques for finding the MAP estimate also only find a <em>local</em>
optimum (which is often good enough), but can fail badly for multimodal
posteriors if the different modes are meaningfully different.</p>
</div>
<div class="section" id="Sampling-methods">
<h4>Sampling methods<a class="headerlink" href="#Sampling-methods" title="Permalink to this headline">¶</a></h4>
<p>Though finding the MAP is a fast and easy way of obtaining estimates of
the unknown model parameters, it is limited because there is no
associated estimate of uncertainty produced with the MAP estimates.
Instead, a simulation-based approach such as Markov chain Monte Carlo
(MCMC) can be used to obtain a Markov chain of values that, given the
satisfaction of certain conditions, are indistinguishable from samples
from the posterior distribution.</p>
<p>To conduct MCMC sampling to generate posterior samples in PyMC3, we
specify a <strong>step method</strong> object that corresponds to a particular MCMC
algorithm, such as Metropolis, Slice sampling, or the No-U-Turn Sampler
(NUTS). PyMC3&#8217;s <code class="docutils literal"><span class="pre">step_methods</span></code> submodule contains the following
samplers: <code class="docutils literal"><span class="pre">NUTS</span></code>, <code class="docutils literal"><span class="pre">Metropolis</span></code>, <code class="docutils literal"><span class="pre">Slice</span></code>, <code class="docutils literal"><span class="pre">HamiltonianMC</span></code>, and
<code class="docutils literal"><span class="pre">BinaryMetropolis</span></code>. These step methods can be assigned manually, or
assigned automatically by PyMC3. Auto-assignment is based on the
attributes of each variable in the model. In general:</p>
<ul class="simple">
<li>Binary variables will be assigned to <code class="docutils literal"><span class="pre">BinaryMetropolis</span></code></li>
<li>Discrete variables will be assigned to <code class="docutils literal"><span class="pre">Metropolis</span></code></li>
<li>Continuous variables will be assigned to <code class="docutils literal"><span class="pre">NUTS</span></code></li>
</ul>
<p>Auto-assignment can be overriden for any subset of variables by
specifying them manually prior to sampling.</p>
</div>
<div class="section" id="Gradient-based-sampling-methods">
<h4>Gradient-based sampling methods<a class="headerlink" href="#Gradient-based-sampling-methods" title="Permalink to this headline">¶</a></h4>
<p>PyMC3 has the standard sampling algorithms like adaptive
Metropolis-Hastings and adaptive slice sampling, but PyMC3&#8217;s most
capable step method is the No-U-Turn Sampler. NUTS is especially useful
on models that have many continuous parameters, a situation where other
MCMC algorithms work very slowly. It takes advantage of information
about where regions of higher probability are, based on the gradient of
the log posterior-density. This helps it achieve dramatically faster
convergence on large problems than traditional sampling methods achieve.
PyMC3 relies on Theano to analytically compute model gradients via
automatic differentiation of the posterior density. NUTS also has
several self-tuning strategies for adaptively setting the tunable
parameters of Hamiltonian Monte Carlo. For random variables that are
undifferentiable (namely, discrete variables) NUTS cannot be used, but
it may still be used on the differentiable variables in a model that
contains undifferentiable variables.</p>
<p>NUTS requires a scaling matrix parameter, which is analogous to the
variance parameter for the jump proposal distribution in
Metropolis-Hastings, although NUTS uses it somewhat differently. The
matrix gives the rough shape of the distribution so that NUTS does not
make jumps that are too large in some directions and too small in other
directions. It is important to set this scaling parameter to a
reasonable value to facilitate efficient sampling. This is especially
true for models that have many unobserved stochastic random variables or
models with highly non-normal posterior distributions. Poor scaling
parameters will slow down NUTS significantly, sometimes almost stopping
it completely. A reasonable starting point for sampling can also be
important for efficient sampling, but not as often.</p>
<p>Fortunately NUTS can often make good guesses for the scaling parameters.
If you pass a point in parameter space (as a dictionary of variable
names to parameter values, the same format as returned by <code class="docutils literal"><span class="pre">find_MAP</span></code>)
to NUTS, it will look at the local curvature of the log
posterior-density (the diagonal of the Hessian matrix) at that point to
make a guess for a good scaling vector, which often results in a good
value. The MAP estimate is often a good point to use to initiate
sampling. It is also possible to supply your own vector or scaling
matrix to NUTS, though this is a more advanced use. If you wish to
modify a Hessian at a specific point to use as your scaling matrix or
vector, you can use <code class="docutils literal"><span class="pre">find_hessian</span></code> or <code class="docutils literal"><span class="pre">find_hessian_diag</span></code>.</p>
<p>For our basic linear regression example in <code class="docutils literal"><span class="pre">basic_model</span></code>, we will use
NUTS to sample 2000 draws from the posterior using the MAP as the
starting point and scaling point. This must also be performed inside the
context of the model.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pymc3</span> <span class="kn">import</span> <span class="n">NUTS</span><span class="p">,</span> <span class="n">sample</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">optimize</span>

<span class="k">with</span> <span class="n">basic_model</span><span class="p">:</span>

    <span class="c1"># obtain starting values via MAP</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">find_MAP</span><span class="p">(</span><span class="n">fmin</span><span class="o">=</span><span class="n">optimize</span><span class="o">.</span><span class="n">fmin_powell</span><span class="p">)</span>

    <span class="c1"># draw 2000 posterior samples</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="n">start</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
Assigned NUTS to alpha
Assigned NUTS to beta
Assigned NUTS to sigma_log_
100%|██████████| 2000/2000 [00:01&lt;00:00, 1314.85it/s]
</pre></div></div>
</div>
<p>The <code class="docutils literal"><span class="pre">sample</span></code> function runs the step method(s) assigned (or passed) to
it for the given number of iterations and returns a <code class="docutils literal"><span class="pre">Trace</span></code> object
containing the samples collected, in the order they were collected. The
<code class="docutils literal"><span class="pre">trace</span></code> object can be queried in a similar way to a <code class="docutils literal"><span class="pre">dict</span></code>
containing a map from variable names to <code class="docutils literal"><span class="pre">numpy.array</span></code>s. The first
dimension of the array is the sampling index and the later dimensions
match the shape of the variable. We can see the last 5 values for the
<code class="docutils literal"><span class="pre">alpha</span></code> variable as follows:</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">trace</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">5</span><span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>Out[9]:
</pre></div>
</div>
<div class="highlight-none"><div class="highlight"><pre>
<span></span>array([ 0.94673726,  0.77287768,  1.03487944,  0.78195843,  1.03158494])
</pre></div>
</div>
</div>
<p>If we wanted to use the slice sampling algorithm to <code class="docutils literal"><span class="pre">sigma</span></code> instead of
NUTS (which was assigned automatically), we could have specified this as
the <code class="docutils literal"><span class="pre">step</span></code> argument for <code class="docutils literal"><span class="pre">sample</span></code>.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pymc3</span> <span class="kn">import</span> <span class="n">Slice</span>

<span class="k">with</span> <span class="n">basic_model</span><span class="p">:</span>

    <span class="c1"># obtain starting values via MAP</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">find_MAP</span><span class="p">(</span><span class="n">fmin</span><span class="o">=</span><span class="n">optimize</span><span class="o">.</span><span class="n">fmin_powell</span><span class="p">)</span>

    <span class="c1"># instantiate sampler</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">Slice</span><span class="p">(</span><span class="nb">vars</span><span class="o">=</span><span class="p">[</span><span class="n">sigma</span><span class="p">])</span>

    <span class="c1"># draw 5000 posterior samples</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="n">start</span><span class="p">)</span>

</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
Assigned NUTS to alpha
Assigned NUTS to beta
100%|██████████| 5000/5000 [00:05&lt;00:00, 894.31it/s]
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="Posterior-analysis">
<h3>Posterior analysis<a class="headerlink" href="#Posterior-analysis" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal"><span class="pre">PyMC3</span></code> provides plotting and summarization functions for inspecting
the sampling output. A simple posterior plot can be created using
<code class="docutils literal"><span class="pre">traceplot</span></code>.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pymc3</span> <span class="kn">import</span> <span class="n">traceplot</span>

<span class="n">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="../_images/notebooks_getting_started_26_0.png" src="../_images/notebooks_getting_started_26_0.png" />
</div>
</div>
<p>The left column consists of a smoothed histogram (using kernel density
estimation) of the marginal posteriors of each stochastic random
variable while the right column contains the samples of the Markov chain
plotted in sequential order. The <code class="docutils literal"><span class="pre">beta</span></code> variable, being vector-valued,
produces two histograms and two sample traces, corresponding to both
predictor coefficients.</p>
<p>In addition, the <code class="docutils literal"><span class="pre">summary</span></code> function provides a text-based output of
common posterior statistics:</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [12]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pymc3</span> <span class="kn">import</span> <span class="n">summary</span>

<span class="n">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>

alpha:

  Mean             SD               MC Error         95% HPD interval
  -------------------------------------------------------------------

  0.907            0.099            0.001            [0.713, 1.105]

  Posterior quantiles:
  2.5            25             50             75             97.5
  |--------------|==============|==============|--------------|

  0.711          0.839          0.906          0.972          1.104


beta:

  Mean             SD               MC Error         95% HPD interval
  -------------------------------------------------------------------

  0.948            0.086            0.001            [0.781, 1.114]
  2.614            0.513            0.005            [1.624, 3.659]

  Posterior quantiles:
  2.5            25             50             75             97.5
  |--------------|==============|==============|--------------|

  0.780          0.890          0.949          1.009          1.114
  1.591          2.279          2.613          2.959          3.633


sigma:

  Mean             SD               MC Error         95% HPD interval
  -------------------------------------------------------------------

  0.990            0.072            0.001            [0.852, 1.130]

  Posterior quantiles:
  2.5            25             50             75             97.5
  |--------------|==============|==============|--------------|

  0.861          0.938          0.985          1.038          1.143

</pre></div></div>
</div>
</div>
</div>
<div class="section" id="Case-study-1:-Stochastic-volatility">
<h2>Case study 1: Stochastic volatility<a class="headerlink" href="#Case-study-1:-Stochastic-volatility" title="Permalink to this headline">¶</a></h2>
<p>We present a case study of stochastic volatility, time varying stock
market volatility, to illustrate PyMC3&#8217;s use in addressing a more
realistic problem. The distribution of market returns is highly
non-normal, which makes sampling the volatilities significantly more
difficult. This example has 400+ parameters so using common sampling
algorithms like Metropolis-Hastings would get bogged down, generating
highly autocorrelated samples. Instead, we use NUTS, which is
dramatically more efficient.</p>
<div class="section" id="The-Model">
<h3>The Model<a class="headerlink" href="#The-Model" title="Permalink to this headline">¶</a></h3>
<p>Asset prices have time-varying volatility (variance of day over day
<code class="docutils literal"><span class="pre">returns</span></code>). In some periods, returns are highly variable, while in
others they are very stable. Stochastic volatility models address this
with a latent volatility variable, which changes over time. The
following model is similar to the one described in the NUTS paper
(Hoffman 2014, p. 21).</p>
<div class="math">
\begin{aligned}
  \sigma &amp;\sim exp(50) \\
  \nu &amp;\sim exp(.1) \\
  s_i &amp;\sim \mathcal{N}(s_{i-1}, \sigma^{-2}) \\
  log(y_i) &amp;\sim t(\nu, 0, exp(-2 s_i))
\end{aligned}</div><p>Here, <span class="math">\(y\)</span> is the daily return series which is modeled with a
Student-t distribution with an unknown degrees of freedom parameter, and
a scale parameter determined by a latent process <span class="math">\(s\)</span>. The
individual <span class="math">\(s_i\)</span> are the individual daily log volatilities in the
latent log volatility process.</p>
</div>
<div class="section" id="The-Data">
<h3>The Data<a class="headerlink" href="#The-Data" title="Permalink to this headline">¶</a></h3>
<p>Our data consist of daily returns of the S&amp;P 500 during the 2008
financial crisis. Here, we use <code class="docutils literal"><span class="pre">pandas-datareader</span></code> to obtain the price
data from Yahoo!-Finance; it can be installed with
<code class="docutils literal"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">pandas-datareader</span></code>.</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [13]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">pandas_datareader</span> <span class="kn">import</span> <span class="n">data</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="o">!</span>pip install pandas-datareader
    <span class="kn">from</span> <span class="nn">pandas_datareader</span> <span class="kn">import</span> <span class="n">data</span>
</pre></div>
</div>
</div>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [14]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">returns</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_data_yahoo</span><span class="p">(</span><span class="s1">&#39;SPY&#39;</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="s1">&#39;2008-5-1&#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;2009-12-1&#39;</span><span class="p">)[</span><span class="s1">&#39;Adj Close&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">pct_change</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">returns</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
401
</pre></div></div>
</div>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [15]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">returns</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;daily returns in %&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="../_images/notebooks_getting_started_33_0.png" src="../_images/notebooks_getting_started_33_0.png" />
</div>
</div>
</div>
<div class="section" id="Model-Specification">
<h3>Model Specification<a class="headerlink" href="#Model-Specification" title="Permalink to this headline">¶</a></h3>
<p>As with the linear regression example, specifying the model in PyMC3
mirrors its statistical specification. This model employs several new
distributions: the <code class="docutils literal"><span class="pre">Exponential</span></code> distribution for the $
<a href="#id2"><span class="problematic" id="id3">:raw-latex:`\nu `$ and :math:`</span></a>sigma` priors, the Student-T
(<code class="docutils literal"><span class="pre">StudentT</span></code>) distribution for distribution of returns, and the
<code class="docutils literal"><span class="pre">GaussianRandomWalk</span></code> for the prior for the latent volatilities.</p>
<p>In PyMC3, variables with purely positive priors like <code class="docutils literal"><span class="pre">Exponential</span></code> are
transformed with a log transform. This makes sampling more robust.
Behind the scenes, a variable in the unconstrained space (named
&#8220;variableName_log&#8221;) is added to the model for sampling. In this model
this happens behind the scenes for both the degrees of freedom, <code class="docutils literal"><span class="pre">nu</span></code>,
and the scale parameter for the volatility process, <code class="docutils literal"><span class="pre">sigma</span></code>, since
they both have exponential priors. Variables with priors that constrain
them on two sides, like <code class="docutils literal"><span class="pre">Beta</span></code> or <code class="docutils literal"><span class="pre">Uniform</span></code>, are also transformed to
be unconstrained but with a log odds transform.</p>
<p>Although, unlike model specification in PyMC2, we do not typically
provide starting points for variables at the model specification stage,
we can also provide an initial value for any distribution (called a
&#8220;test value&#8221;) using the <code class="docutils literal"><span class="pre">testval</span></code> argument. This overrides the default
test value for the distribution (usually the mean, median or mode of the
distribution), and is most often useful if some values are illegal and
we want to ensure we select a legal one. The test values for the
distributions are also used as a starting point for sampling and
optimization by default, though this is easily overriden.</p>
<p>The vector of latent volatilities <code class="docutils literal"><span class="pre">s</span></code> is given a prior distribution by
<code class="docutils literal"><span class="pre">GaussianRandomWalk</span></code>. As its name suggests GaussianRandomWalk is a
vector valued distribution where the values of the vector form a random
normal walk of length n, as specified by the <code class="docutils literal"><span class="pre">shape</span></code> argument. The
scale of the innovations of the random walk, <code class="docutils literal"><span class="pre">sigma</span></code>, is specified in
terms of the precision of the normally distributed innovations and can
be a scalar or vector.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [16]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pymc3</span> <span class="kn">import</span> <span class="n">Exponential</span><span class="p">,</span> <span class="n">StudentT</span><span class="p">,</span> <span class="n">Deterministic</span>
<span class="kn">from</span> <span class="nn">pymc3.math</span> <span class="kn">import</span> <span class="n">exp</span>
<span class="kn">from</span> <span class="nn">pymc3.distributions.timeseries</span> <span class="kn">import</span> <span class="n">GaussianRandomWalk</span>

<span class="k">with</span> <span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">sp500_model</span><span class="p">:</span>

    <span class="n">nu</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="s1">&#39;nu&#39;</span><span class="p">,</span> <span class="mf">1.</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="mf">5.</span><span class="p">)</span>

    <span class="n">sigma</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="mf">1.</span><span class="o">/.</span><span class="mo">02</span><span class="p">,</span> <span class="n">testval</span><span class="o">=.</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">GaussianRandomWalk</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">sigma</span><span class="o">**-</span><span class="mi">2</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">returns</span><span class="p">))</span>

    <span class="n">volatility_process</span> <span class="o">=</span> <span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;volatility_process&#39;</span><span class="p">,</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">s</span><span class="p">))</span>

    <span class="n">r</span> <span class="o">=</span> <span class="n">StudentT</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">volatility_process</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">returns</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
Applied log-transform to nu and added transformed nu_log_ to model.
Applied log-transform to sigma and added transformed sigma_log_ to model.
</pre></div></div>
</div>
<p>Notice that we transform the log volatility process <code class="docutils literal"><span class="pre">s</span></code> into the
volatility process by <code class="docutils literal"><span class="pre">exp(-2*s)</span></code>. Here, <code class="docutils literal"><span class="pre">exp</span></code> is a Theano function,
rather than the corresponding function in NumPy; Theano provides a large
subset of the mathematical functions that NumPy does.</p>
<p>Also note that we have declared the <code class="docutils literal"><span class="pre">Model</span></code> name <code class="docutils literal"><span class="pre">sp500_model</span></code> in
the first occurrence of the context manager, rather than splitting it
into two lines, as we did for the first example.</p>
</div>
<div class="section" id="Fitting">
<h3>Fitting<a class="headerlink" href="#Fitting" title="Permalink to this headline">¶</a></h3>
<p>Before we draw samples from the posterior, it is prudent to find a
decent starting value by finding a point of relatively high probability.
For this model, the full <em>maximum a posteriori</em> (MAP) point over all
variables is degenerate and has infinite density. But, if we fix
<code class="docutils literal"><span class="pre">log_sigma</span></code> and <code class="docutils literal"><span class="pre">nu</span></code> it is no longer degenerate, so we find the MAP
with respect only to the volatility process <code class="docutils literal"><span class="pre">s</span></code> keeping <code class="docutils literal"><span class="pre">log_sigma</span></code>
and <code class="docutils literal"><span class="pre">nu</span></code> constant at their default values (remember that we set
<code class="docutils literal"><span class="pre">testval=.1</span></code> for <code class="docutils literal"><span class="pre">sigma</span></code>). We use the Limited-memory BFGS (L-BFGS)
optimizer, which is provided by the <code class="docutils literal"><span class="pre">scipy.optimize</span></code> package, as it is
more efficient for high dimensional functions and we have 400 stochastic
random variables (mostly from <code class="docutils literal"><span class="pre">s</span></code>).</p>
<p>To achieve good convergence with NUTS, it is critical to find a good
scaling. The MAP as found by <code class="docutils literal"><span class="pre">find_MAP()</span></code> is in our experience a poor
choice. Instead, we run automatic differentiation variational inference
(ADVI) to get a variational estimate of the posterior mean as well as
the posterior standard deviations. We can use these estimates to
initialize NUTS to achieve faster sampling due to better scaling.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [17]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pymc3</span> <span class="kn">import</span> <span class="n">variational</span>

<span class="kn">import</span> <span class="nn">scipy</span>
<span class="k">with</span> <span class="n">sp500_model</span><span class="p">:</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">sds</span><span class="p">,</span> <span class="n">elbo</span> <span class="o">=</span> <span class="n">variational</span><span class="o">.</span><span class="n">advi</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">scaling</span><span class="o">=</span><span class="n">sp500_model</span><span class="o">.</span><span class="n">dict_to_array</span><span class="p">(</span><span class="n">sds</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">is_cov</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">trace</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">progressbar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
Iteration 0 [0%]: ELBO = -60151.01
Iteration 10000 [10%]: Average ELBO = -32763.07
Iteration 20000 [20%]: Average ELBO = -266.02
Iteration 30000 [30%]: Average ELBO = 130.94
Iteration 40000 [40%]: Average ELBO = 520.74
Iteration 50000 [50%]: Average ELBO = 767.25
Iteration 60000 [60%]: Average ELBO = 853.8
Iteration 70000 [70%]: Average ELBO = 874.46
Iteration 80000 [80%]: Average ELBO = 879.28
Iteration 90000 [90%]: Average ELBO = 881.4
Finished [100%]: Average ELBO = 882.11
</pre></div></div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
100%|██████████| 2000/2000 [02:09&lt;00:00, 11.58it/s]
</pre></div></div>
</div>
<p>We can check our samples by looking at the traceplot for <code class="docutils literal"><span class="pre">nu</span></code> and
<code class="docutils literal"><span class="pre">sigma</span></code>.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [18]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">[</span><span class="mi">200</span><span class="p">:],</span> <span class="p">[</span><span class="n">nu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="../_images/notebooks_getting_started_40_0.png" src="../_images/notebooks_getting_started_40_0.png" />
</div>
</div>
<p>Finally we plot the distribution of volatility paths by plotting many of
our sampled volatility paths on the same graph. Each is rendered
partially transparent (via the <code class="docutils literal"><span class="pre">alpha</span></code> argument in Matplotlib&#8217;s
<code class="docutils literal"><span class="pre">plot</span></code> function) so the regions where many paths overlap are shaded
more darkly.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [19]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">returns</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">returns</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">trace</span><span class="p">[</span><span class="s1">&#39;s&#39;</span><span class="p">,::</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mo">03</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;volatility_process&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;time&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;volatility&#39;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;S&amp;P500&#39;</span><span class="p">,</span> <span class="s1">&#39;stochastic volatility process&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>Out[19]:
</pre></div>
</div>
<div class="highlight-none"><div class="highlight"><pre>
<span></span>&lt;matplotlib.legend.Legend at 0x7f7ca5a0b438&gt;
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="../_images/notebooks_getting_started_42_1.png" src="../_images/notebooks_getting_started_42_1.png" />
</div>
</div>
<p>As you can see, the model correctly infers the increase in volatility
during the 2008 financial crash. Moreover, note that this model is quite
complex because of its high dimensionality and dependency-structure in
the random walk distribution. NUTS as implemented in PyMC3, however,
correctly infers the posterior distribution with ease.</p>
</div>
</div>
<div class="section" id="Case-study-2:-Coal-mining-disasters">
<h2>Case study 2: Coal mining disasters<a class="headerlink" href="#Case-study-2:-Coal-mining-disasters" title="Permalink to this headline">¶</a></h2>
<p>Consider the following time series of recorded coal mining disasters in
the UK from 1851 to 1962 (Jarrett, 1979). The number of disasters is
thought to have been affected by changes in safety regulations during
this period. Unfortunately, we also have pair of years with missing
data, identified as missing by a NumPy MaskedArray using -999 as the
marker value.</p>
<p>Next we will build a model for this series and attempt to estimate when
the change occurred. At the same time, we will see how to handle missing
data, use multiple samplers and sample from discrete random variables.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [20]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">disaster_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">masked_values</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span>
                            <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span>
                            <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">999</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
                            <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                            <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span>
                            <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">999</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span>
                            <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">value</span><span class="o">=-</span><span class="mi">999</span><span class="p">)</span>
<span class="n">year</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1851</span><span class="p">,</span> <span class="mi">1962</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">year</span><span class="p">,</span> <span class="n">disaster_data</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Disaster count&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Year&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>Out[20]:
</pre></div>
</div>
<div class="highlight-none"><div class="highlight"><pre>
<span></span>&lt;matplotlib.text.Text at 0x7f7ca087aac8&gt;
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="../_images/notebooks_getting_started_45_1.png" src="../_images/notebooks_getting_started_45_1.png" />
</div>
</div>
<p>Occurrences of disasters in the time series is thought to follow a
Poisson process with a large rate parameter in the early part of the
time series, and from one with a smaller rate in the later part. We are
interested in locating the change point in the series, which perhaps is
related to changes in mining safety regulations.</p>
<p>In our model,</p>
<div class="math">
\[ \begin{align}\begin{aligned}:nowrap:\\\begin{split}  \begin{aligned}
    D_t &amp;\sim \text{Pois}(r_t), r_t= \begin{cases}
     l, &amp; \text{if } t \lt s \\
     e, &amp; \text{if } t \ge s
     \end{cases} \\
    s &amp;\sim \text{Unif}(t_l, t_h)\\
    e &amp;\sim \text{exp}(1)\\
    l &amp;\sim \text{exp}(1)
  \end{aligned}\end{split}\\the parameters are defined as follows: \* :math:`D_t`: The number of\end{aligned}\end{align} \]</div>
<p>disasters in year <span class="math">\(t\)</span> * <span class="math">\(r_t\)</span>: The rate parameter of the
Poisson distribution of disasters in year <span class="math">\(t\)</span>. * <span class="math">\(s\)</span>: The
year in which the rate parameter changes (the switchpoint). *
<span class="math">\(e\)</span>: The rate parameter before the switchpoint <span class="math">\(s\)</span>. *
<span class="math">\(l\)</span>: The rate parameter after the switchpoint <span class="math">\(s\)</span>. *
<span class="math">\(t_l\)</span>, <span class="math">\(t_h\)</span>: The lower and upper boundaries of year
<span class="math">\(t\)</span>.</p>
<p>This model is built much like our previous models. The major differences
are the introduction of discrete variables with the Poisson and
discrete-uniform priors and the novel form of the deterministic random
variable <code class="docutils literal"><span class="pre">rate</span></code>.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [21]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pymc3</span> <span class="kn">import</span> <span class="n">DiscreteUniform</span><span class="p">,</span> <span class="n">Poisson</span>
<span class="kn">from</span> <span class="nn">pymc3.math</span> <span class="kn">import</span> <span class="n">switch</span>

<span class="k">with</span> <span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">disaster_model</span><span class="p">:</span>

    <span class="n">switchpoint</span> <span class="o">=</span> <span class="n">DiscreteUniform</span><span class="p">(</span><span class="s1">&#39;switchpoint&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="n">year</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">upper</span><span class="o">=</span><span class="n">year</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">testval</span><span class="o">=</span><span class="mi">1900</span><span class="p">)</span>

    <span class="c1"># Priors for pre- and post-switch rates number of disasters</span>
    <span class="n">early_rate</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="s1">&#39;early_rate&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">late_rate</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="s1">&#39;late_rate&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Allocate appropriate Poisson rates to years before and after current</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="n">switch</span><span class="p">(</span><span class="n">switchpoint</span> <span class="o">&gt;=</span> <span class="n">year</span><span class="p">,</span> <span class="n">early_rate</span><span class="p">,</span> <span class="n">late_rate</span><span class="p">)</span>

    <span class="n">disasters</span> <span class="o">=</span> <span class="n">Poisson</span><span class="p">(</span><span class="s1">&#39;disasters&#39;</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">disaster_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
Applied log-transform to early_rate and added transformed early_rate_log_ to model.
Applied log-transform to late_rate and added transformed late_rate_log_ to model.
</pre></div></div>
</div>
<p>The logic for the rate random variable,</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">rate</span> <span class="o">=</span> <span class="n">switch</span><span class="p">(</span><span class="n">switchpoint</span> <span class="o">&gt;=</span> <span class="n">year</span><span class="p">,</span> <span class="n">early_rate</span><span class="p">,</span> <span class="n">late_rate</span><span class="p">)</span>
</pre></div>
</div>
<p>is implemented using <code class="docutils literal"><span class="pre">switch</span></code>, a Theano function that works like an if
statement. It uses the first argument to switch between the next two
arguments.</p>
<p>Missing values are handled transparently by passing a <code class="docutils literal"><span class="pre">MaskedArray</span></code> or
a <code class="docutils literal"><span class="pre">pandas.DataFrame</span></code> with NaN values to the <code class="docutils literal"><span class="pre">observed</span></code> argument when
creating an observed stochastic random variable. Behind the scenes,
another random variable, <code class="docutils literal"><span class="pre">disasters.missing_values</span></code> is created to
model the missing values. All we need to do to handle the missing values
is ensure we sample this random variable as well.</p>
<p>Unfortunately because they are discrete variables and thus have no
meaningful gradient, we cannot use NUTS for sampling <code class="docutils literal"><span class="pre">switchpoint</span></code> or
the missing disaster observations. Instead, we will sample using a
<code class="docutils literal"><span class="pre">Metroplis</span></code> step method, which implements adaptive
Metropolis-Hastings, because it is designed to handle discrete values.</p>
<p>We sample with both samplers at once by passing them to the <code class="docutils literal"><span class="pre">sample</span></code>
function in a list. Each new sample is generated by first applying
<code class="docutils literal"><span class="pre">step1</span></code> then <code class="docutils literal"><span class="pre">step2</span></code>.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [22]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pymc3</span> <span class="kn">import</span> <span class="n">Metropolis</span>

<span class="k">with</span> <span class="n">disaster_model</span><span class="p">:</span>
    <span class="n">step1</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">([</span><span class="n">early_rate</span><span class="p">,</span> <span class="n">late_rate</span><span class="p">])</span>

    <span class="c1"># Use Metropolis for switchpoint, and missing values since it accommodates discrete variables</span>
    <span class="n">step2</span> <span class="o">=</span> <span class="n">Metropolis</span><span class="p">([</span><span class="n">switchpoint</span><span class="p">,</span> <span class="n">disasters</span><span class="o">.</span><span class="n">missing_values</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>

    <span class="n">trace</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="p">[</span><span class="n">step1</span><span class="p">,</span> <span class="n">step2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
100%|██████████| 10000/10000 [00:07&lt;00:00, 1277.29it/s]
</pre></div></div>
</div>
<p>In the trace plot below we can see that there&#8217;s about a 10 year span
that&#8217;s plausible for a significant change in safety, but a 5 year span
that contains most of the probability mass. The distribution is jagged
because of the jumpy relationship between the year switchpoint and the
likelihood and not due to sampling error.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [23]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="../_images/notebooks_getting_started_52_0.png" src="../_images/notebooks_getting_started_52_0.png" />
</div>
</div>
</div>
<div class="section" id="Arbitrary-deterministics">
<h2>Arbitrary deterministics<a class="headerlink" href="#Arbitrary-deterministics" title="Permalink to this headline">¶</a></h2>
<p>Due to its reliance on Theano, PyMC3 provides many mathematical
functions and operators for transforming random variables into new
random variables. However, the library of functions in Theano is not
exhaustive, therefore Theano and PyMC3 provide functionality for
creating arbitrary Theano functions in pure Python, and including these
functions in PyMC models. This is supported with the <code class="docutils literal"><span class="pre">as_op</span></code> function
decorator.</p>
<p>Theano needs to know the types of the inputs and outputs of a function,
which are specified for <code class="docutils literal"><span class="pre">as_op</span></code> by <code class="docutils literal"><span class="pre">itypes</span></code> for inputs and
<code class="docutils literal"><span class="pre">otypes</span></code> for outputs. The Theano documentation includes <a class="reference external" href="http://deeplearning.net/software/theano/library/tensor/basic.html#all-fully-typed-constructors">an overview
of the available
types</a>.</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [24]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">T</span>
<span class="kn">from</span> <span class="nn">theano.compile.ops</span> <span class="kn">import</span> <span class="n">as_op</span>

<span class="nd">@as_op</span><span class="p">(</span><span class="n">itypes</span><span class="o">=</span><span class="p">[</span><span class="n">T</span><span class="o">.</span><span class="n">lscalar</span><span class="p">],</span> <span class="n">otypes</span><span class="o">=</span><span class="p">[</span><span class="n">T</span><span class="o">.</span><span class="n">lscalar</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">crazy_modulo3</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">value</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">value</span> <span class="o">%</span> <span class="mi">3</span>
    <span class="k">else</span> <span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="n">value</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span>

<span class="k">with</span> <span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_deterministic</span><span class="p">:</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">Poisson</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">crazy_modulo3</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>An important drawback of this approach is that it is not possible for
<code class="docutils literal"><span class="pre">theano</span></code> to inspect these functions in order to compute the gradient
required for the Hamiltonian-based samplers. Therefore, it is not
possible to use the HMC or NUTS samplers for a model that uses such an
operator. However, it is possible to add a gradient if we inherit from
<code class="docutils literal"><span class="pre">theano.Op</span></code> instead of using <code class="docutils literal"><span class="pre">as_op</span></code>. The PyMC example set includes
<a class="reference external" href="https://github.com/pymc-devs/pymc3/blob/master/pymc3/examples/disaster_model_arbitrary_deterministic.py">a more elaborate example of the usage of
as_op</a>.</p>
</div>
<div class="section" id="Arbitrary-distributions">
<h2>Arbitrary distributions<a class="headerlink" href="#Arbitrary-distributions" title="Permalink to this headline">¶</a></h2>
<p>Similarly, the library of statistical distributions in PyMC3 is not
exhaustive, but PyMC allows for the creation of user-defined functions
for an arbitrary probability distribution. For simple statistical
distributions, the <code class="docutils literal"><span class="pre">DensityDist</span></code> function takes as an argument any
function that calculates a log-probability <span class="math">\(log(p(x))\)</span>. This
function may employ other random variables in its calculation. Here is
an example inspired by a blog post by Jake Vanderplas on which priors to
use for a linear regression (Vanderplas, 2014).</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">T</span>
<span class="kn">from</span> <span class="nn">pymc3</span> <span class="kn">import</span> <span class="n">DensityDist</span><span class="p">,</span> <span class="n">Uniform</span>

<span class="k">with</span> <span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;intercept&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

    <span class="c1"># Create custom densities</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">DensityDist</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">value</span><span class="p">:</span> <span class="o">-</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">value</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">testval</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">DensityDist</span><span class="p">(</span><span class="s1">&#39;eps&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">value</span><span class="p">:</span> <span class="o">-</span><span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">abs_</span><span class="p">(</span><span class="n">value</span><span class="p">)),</span> <span class="n">testval</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Create likelihood</span>
    <span class="n">like</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y_est&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">X</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p>For more complex distributions, one can create a subclass of
<code class="docutils literal"><span class="pre">Continuous</span></code> or <code class="docutils literal"><span class="pre">Discrete</span></code> and provide the custom <code class="docutils literal"><span class="pre">logp</span></code> function,
as required. This is how the built-in distributions in PyMC are
specified. As an example, fields like psychology and astrophysics have
complex likelihood functions for a particular process that may require
numerical approximation. In these cases, it is impossible to write the
function in terms of predefined theano operators and we must use a
custom theano operator using <code class="docutils literal"><span class="pre">as_op</span></code> or inheriting from <code class="docutils literal"><span class="pre">theano.Op</span></code>.</p>
<p>Implementing the <code class="docutils literal"><span class="pre">beta</span></code> variable above as a <code class="docutils literal"><span class="pre">Continuous</span></code> subclass is
shown below, along with a sub-function using the <code class="docutils literal"><span class="pre">as_op</span></code> decorator,
though this is not strictly necessary.</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [25]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pymc3.distributions</span> <span class="kn">import</span> <span class="n">Continuous</span>

<span class="k">class</span> <span class="nc">Beta</span><span class="p">(</span><span class="n">Continuous</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Beta</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mu</span>

    <span class="k">def</span> <span class="nf">logp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span>
        <span class="k">return</span> <span class="n">beta_logp</span><span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span>

<span class="nd">@as_op</span><span class="p">(</span><span class="n">itypes</span><span class="o">=</span><span class="p">[</span><span class="n">T</span><span class="o">.</span><span class="n">dscalar</span><span class="p">],</span> <span class="n">otypes</span><span class="o">=</span><span class="p">[</span><span class="n">T</span><span class="o">.</span><span class="n">dscalar</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">beta_logp</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>


<span class="k">with</span> <span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">Beta</span><span class="p">(</span><span class="s1">&#39;slope&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Generalized-Linear-Models">
<h2>Generalized Linear Models<a class="headerlink" href="#Generalized-Linear-Models" title="Permalink to this headline">¶</a></h2>
<p>Generalized Linear Models (GLMs) are a class of flexible models that are
widely used to estimate regression relationships between a single
outcome variable and one or multiple predictors. Because these models
are so common, <code class="docutils literal"><span class="pre">PyMC3</span></code> offers a <code class="docutils literal"><span class="pre">glm</span></code> submodule that allows flexible
creation of various GLMs with an intuitive <code class="docutils literal"><span class="pre">R</span></code>-like syntax that is
implemented via the <code class="docutils literal"><span class="pre">patsy</span></code> module.</p>
<p>The <code class="docutils literal"><span class="pre">glm</span></code> submodule requires data to be included as a <code class="docutils literal"><span class="pre">pandas</span></code>
<code class="docutils literal"><span class="pre">DataFrame</span></code>. Hence, for our linear regression example:</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [26]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># Convert X and Y to a pandas DataFrame</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;x1&#39;</span><span class="p">:</span> <span class="n">X1</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">:</span> <span class="n">X2</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">Y</span><span class="p">})</span>
</pre></div>
</div>
</div>
<p>The model can then be very concisely specified in one line of code.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [27]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pymc3.glm</span> <span class="kn">import</span> <span class="n">glm</span>

<span class="k">with</span> <span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_glm</span><span class="p">:</span>
    <span class="n">glm</span><span class="p">(</span><span class="s1">&#39;y ~ x1 + x2&#39;</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
Applied log-transform to sd and added transformed sd_log_ to model.
Assigned NUTS to Intercept
Assigned NUTS to x1
Assigned NUTS to x2
Assigned NUTS to sd_log_
100%|██████████| 5000/5000 [00:04&lt;00:00, 1203.77it/s]
</pre></div></div>
</div>
<p>The error distribution, if not specified via the <code class="docutils literal"><span class="pre">family</span></code> argument, is
assumed to be normal. In the case of logistic regression, this can be
modified by passing in a <code class="docutils literal"><span class="pre">Binomial</span></code> family object.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [28]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pymc3.glm.families</span> <span class="kn">import</span> <span class="n">Binomial</span>

<span class="n">df_logistic</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;x1&#39;</span><span class="p">:</span> <span class="n">X1</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">Y</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">Y</span><span class="p">)})</span>

<span class="k">with</span> <span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_glm_logistic</span><span class="p">:</span>
    <span class="n">glm</span><span class="p">(</span><span class="s1">&#39;y ~ x1&#39;</span><span class="p">,</span> <span class="n">df_logistic</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">Binomial</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
Applied logodds-transform to p and added transformed p_logodds_ to model.
</pre></div></div>
</div>
</div>
<div class="section" id="Backends">
<h2>Backends<a class="headerlink" href="#Backends" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal"><span class="pre">PyMC3</span></code> has support for different ways to store samples during and
after sampling, called backends, including in-memory (default), text
file, and SQLite. These can be found in <code class="docutils literal"><span class="pre">pymc.backends</span></code>:</p>
<p>By default, an in-memory <code class="docutils literal"><span class="pre">ndarray</span></code> is used but if the samples would
get too large to be held in memory we could use the <code class="docutils literal"><span class="pre">sqlite</span></code> backend:</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [29]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pymc3.backends</span> <span class="kn">import</span> <span class="n">SQLite</span>

<span class="k">with</span> <span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_glm_logistic</span><span class="p">:</span>
    <span class="n">glm</span><span class="p">(</span><span class="s1">&#39;y ~ x1&#39;</span><span class="p">,</span> <span class="n">df_logistic</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">Binomial</span><span class="p">())</span>

    <span class="n">backend</span> <span class="o">=</span> <span class="n">SQLite</span><span class="p">(</span><span class="s1">&#39;trace.sqlite&#39;</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">find_MAP</span><span class="p">()</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">scaling</span><span class="o">=</span><span class="n">start</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="n">start</span><span class="p">,</span> <span class="n">trace</span><span class="o">=</span><span class="n">backend</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
Applied logodds-transform to p and added transformed p_logodds_ to model.
100%|██████████| 5000/5000 [00:04&lt;00:00, 1138.49it/s]
</pre></div></div>
</div>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [30]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">varnames</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>

x1:

  Mean             SD               MC Error         95% HPD interval
  -------------------------------------------------------------------

  0.001            0.130            0.001            [-0.244, 0.263]

  Posterior quantiles:
  2.5            25             50             75             97.5
  |--------------|==============|==============|--------------|

  -0.251         -0.088         0.001          0.090          0.257

</pre></div></div>
</div>
<p>The stored trace can then later be loaded using the <code class="docutils literal"><span class="pre">load</span></code> command:</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [31]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pymc3.backends.sqlite</span> <span class="kn">import</span> <span class="n">load</span>

<span class="k">with</span> <span class="n">basic_model</span><span class="p">:</span>
    <span class="n">trace_loaded</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s1">&#39;trace.sqlite&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>More information about <code class="docutils literal"><span class="pre">backends</span></code> can be found in the docstring of
<code class="docutils literal"><span class="pre">pymc.backends</span></code>.</p>
</div>
<div class="section" id="Discussion">
<h2>Discussion<a class="headerlink" href="#Discussion" title="Permalink to this headline">¶</a></h2>
<p>Probabilistic programming is an emerging paradigm in statistical
learning, of which Bayesian modeling is an important sub-discipline. The
signature characteristics of probabilistic programming&#8211;specifying
variables as probability distributions and conditioning variables on
other variables and on observations&#8211;makes it a powerful tool for
building models in a variety of settings, and over a range of model
complexity. Accompanying the rise of probabilistic programming has been
a burst of innovation in fitting methods for Bayesian models that
represent notable improvement over existing MCMC methods. Yet, despite
this expansion, there are few software packages available that have kept
pace with the methodological innovation, and still fewer that allow
non-expert users to implement models.</p>
<p>PyMC3 provides a probabilistic programming platform for quantitative
researchers to implement statistical models flexibly and succinctly. A
large library of statistical distributions and several pre-defined
fitting algorithms allows users to focus on the scientific problem at
hand, rather than the implementation details of Bayesian modeling. The
choice of Python as a development language, rather than a
domain-specific language, means that PyMC3 users are able to work
interactively to build models, introspect model objects, and debug or
profile their work, using a dynamic, high-level programming language
that is easy to learn. The modular, object-oriented design of PyMC3
means that adding new fitting algorithms or other features is
straightforward. In addition, PyMC3 comes with several features not
found in most other packages, most notably Hamiltonian-based samplers as
well as automatical transforms of constrained random variables which is
only offered by STAN. Unlike STAN, however, PyMC3 supports discrete
variables as well as non-gradient based sampling algorithms like
Metropolis-Hastings and Slice sampling.</p>
<p>Development of PyMC3 is an ongoing effort and several features are
planned for future versions. Most notably, variational inference
techniques are often more efficient than MCMC sampling, at the cost of
generalizability. More recently, however, black-box variational
inference algorithms have been developed, such as automatic
differentiation variational inference (ADVI; Kucukelbir et al., in
prep). This algorithm is slated for addition to PyMC3. As an open-source
scientific computing toolkit, we encourage researchers developing new
fitting algorithms for Bayesian models to provide reference
implementations in PyMC3. Since samplers can be written in pure Python
code, they can be implemented generally to make them work on arbitrary
PyMC3 models, giving authors a larger audience to put their methods into
use.</p>
</div>
<div class="section" id="References">
<h2>References<a class="headerlink" href="#References" title="Permalink to this headline">¶</a></h2>
<p>Patil, A., D. Huard and C.J. Fonnesbeck. (2010) PyMC: Bayesian
Stochastic Modelling in Python. Journal of Statistical Software, 35(4),
pp. 1-81</p>
<p>Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I.,
Bergeron, A., Bouchard, N., Warde-Farley, D., and Bengio, Y. (2012)
“Theano: new features and speed improvements”. NIPS 2012 deep learning
workshop.</p>
<p>Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R.,
Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010)
“Theano: A CPU and GPU Math Expression Compiler”. Proceedings of the
Python for Scientific Computing Conference (SciPy) 2010. June 30 - July
3, Austin, TX</p>
<p>Lunn, D.J., Thomas, A., Best, N., and Spiegelhalter, D. (2000) WinBUGS
&#8211; a Bayesian modelling framework: concepts, structure, and
extensibility. Statistics and Computing, 10:325&#8211;337.</p>
<p>Neal, R.M. Slice sampling. Annals of Statistics. (2003).
doi:10.2307/3448413.</p>
<p>van Rossum, G. The Python Library Reference Release 2.6.5., (2010). URL
<a class="reference external" href="http://docs.python.org/library/">http://docs.python.org/library/</a>.</p>
<p>Duane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D. (1987)
“Hybrid Monte Carlo”, Physics Letters, vol. 195, pp. 216-222.</p>
<p>Stan Development Team. (2014). Stan: A C++ Library for Probability and
Sampling, Version 2.5.0. <a class="reference external" href="http://mc-stan.org">http://mc-stan.org</a>.</p>
<p>Gamerman, D. Markov Chain Monte Carlo: statistical simulation for
Bayesian inference. Chapman and Hall, 1997.</p>
<p>Hoffman, M. D., &amp; Gelman, A. (2014). The No-U-Turn Sampler: Adaptively
Setting Path Lengths in Hamiltonian Monte Carlo. The Journal of Machine
Learning Research, 30.</p>
<p>Kucukelbir A, Ranganath R, Gelman A, and Blei DM. Automatic Variational
Inference in Stan <a class="reference external" href="http://arxiv.org/abs/1506.03431">http://arxiv.org/abs/1506.03431</a>, in prep.</p>
<p>Vanderplas, Jake. &#8220;Frequentism and Bayesianism IV: How to be a Bayesian
in Python.&#8221; Pythonic Perambulations. N.p., 14 Jun 2014. Web. 27 May.
2015.
<a class="reference external" href="https://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/">https://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/</a>.</p>
<p>R.G. Jarrett. A note on the intervals between coal mining disasters.
Biometrika, 66:191–193, 1979.</p>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../examples.html" class="btn btn-neutral float-right" title="Examples" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../getting_started.html" class="btn btn-neutral" title="Getting started" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, John Salvatier, Christopher Fonnesbeck, Thomas Wiecki.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'3.0.rc1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>