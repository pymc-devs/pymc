<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>3. Tutorial &mdash; pymc v2.0 documentation</title>
    <link rel="stylesheet" href="_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '2.0',
        COLLAPSE_MODINDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="pymc v2.0 documentation" href="index.html" />
    <link rel="next" title="4. Building models" href="modelbuilding.html" />
    <link rel="prev" title="2. Installation Instructions" href="INSTALL.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="modelbuilding.html" title="4. Building models"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="INSTALL.html" title="2. Installation Instructions"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">pymc v2.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/icon_small.png" alt="Logo"/>
            </a></p>
            <h3><a href="index.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference external" href="">3. Tutorial</a><ul>
<li><a class="reference external" href="#an-example-statistical-model">3.1. An example statistical model</a></li>
<li><a class="reference external" href="#two-types-of-variables">3.2. Two types of variables</a><ul>
<li><a class="reference external" href="#why-are-data-and-unknown-variables-represented-by-the-same-object">3.2.1. Why are data and unknown variables represented by the same object?</a></li>
</ul>
</li>
<li><a class="reference external" href="#parents-and-children">3.3. Parents and children</a></li>
<li><a class="reference external" href="#variables-values-and-log-probabilities">3.4. Variables&#8217; values and log-probabilities</a><ul>
<li><a class="reference external" href="#using-variables-as-parents-of-other-variables">3.4.1. Using Variables as parents of other Variables</a></li>
</ul>
</li>
<li><a class="reference external" href="#fitting-the-model-with-mcmc">3.5. Fitting the model with MCMC</a><ul>
<li><a class="reference external" href="#what-does-it-mean-to-fit-a-model">3.5.1. What does it mean to fit a model?</a></li>
<li><a class="reference external" href="#accessing-the-samples">3.5.2. Accessing the samples</a></li>
<li><a class="reference external" href="#sampling-output">3.5.3. Sampling output</a></li>
<li><a class="reference external" href="#imputation-of-missing-data">3.5.4. Imputation of Missing Data</a></li>
</ul>
</li>
<li><a class="reference external" href="#fine-tuning-the-mcmc-algorithm">3.6. Fine-tuning the MCMC algorithm</a></li>
<li><a class="reference external" href="#beyond-the-basics">3.7. Beyond the basics</a></li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="INSTALL.html"
                                  title="previous chapter">2. Installation Instructions</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="modelbuilding.html"
                                  title="next chapter">4. Building models</a></p>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="_sources/tutorial.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="tutorial">
<h1>3. Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline">¶</a></h1>
<div class="section" id="an-example-statistical-model">
<h2>3.1. An example statistical model<a class="headerlink" href="#an-example-statistical-model" title="Permalink to this headline">¶</a></h2>
<p>Consider the following dataset, which is a time series of recorded coal
mining disasters in the UK from 1851 to 1962 <a class="reference external" href="references.html#jarrett-1979">[Jarrett:1979]</a>.</p>
<div align="center" class="figure" id="disastersts">
<a class="reference external image-reference" href="_images/disastersts_web.png"><img alt="Disasters time series." src="_images/disastersts_web.png" style="width: 480.0px; height: 240.0px;" /></a>
<p class="caption">Number of mining disasters each year in the UK.</p>
</div>
<p>Occurrences of disasters in the time series is thought to be derived from
a Poisson process with a large rate parameter in the early part of the
time series, and from one with a smaller rate in the later part. We are
interested in locating the change point in the series, which perhaps is
related to changes in mining safety regulations.</p>
<p>We represent our conceptual model formally as a statistical model:</p>
<div class="math" id="equation-disastermodel">
<p><span class="eqno">(1)</span><img src="_images/math/c405b48247f4cd26c9b0297c5611da3149417b2f.png" alt="\begin{array}{ccc}
        (D_t | s, e, l) \sim \textup{Poisson}\left(r_t\right), &amp; r_t=\left\{\begin{array}{lll}
            e &amp;\textup{if}&amp; t&lt; s\\ l &amp;\textup{if}&amp; t\ge s
            \end{array}\right.,&amp;t\in[t_l,t_h]\\
        s\sim \textup{Discrete Uniform}(t_l, t_h)\\
        e\sim \textup{Exponential}(r_e)\\
        l\sim \textup{Exponential}(r_l)
    \end{array}" />
</div></p><p>The symbols are defined as:</p>
<ul class="simple">
<li><img class="math" src="_images/math/56ff51eb4870ec165ac7daed4a82c7323caf4e78.png" alt="D_t"/>: The number of disasters in year <img class="math" src="_images/math/e0d2bf360290fd61d1c1557e763f2622363b3d35.png" alt="t"/>.</li>
<li><img class="math" src="_images/math/2c67102ec937a734c3f5b8c00723d32f0008978e.png" alt="r_t"/>: The rate parameter of the Poisson distribution of disasters in year <img class="math" src="_images/math/e0d2bf360290fd61d1c1557e763f2622363b3d35.png" alt="t"/>.</li>
<li><img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>:   The year in which the rate parameter changes (the switchpoint).</li>
<li><img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/>:   The rate parameter before the switchpoint <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>.</li>
<li><img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/>:   The rate parameter after the switchpoint <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>.</li>
<li><img class="math" src="_images/math/8e0b9f92fa1d1f3b8b4c42698703c470dad70615.png" alt="t_l"/>, <img class="math" src="_images/math/cdb0b5d73c0bcf07ab79e7d5929b5f6c2906c75c.png" alt="t_h"/>:    The lower and upper boundaries of year <img class="math" src="_images/math/e0d2bf360290fd61d1c1557e763f2622363b3d35.png" alt="t"/>.</li>
<li><img class="math" src="_images/math/3586216dffe0ec0c67477348b2c3ebd91b489edf.png" alt="r_e"/>, <img class="math" src="_images/math/e6237d428c648e1670c2e4989a7837467d4ec53b.png" alt="r_l"/>:    The rate parameters of the priors of the early and late rates, respectively.</li>
</ul>
<p>Because we have defined <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/> by its dependence on <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>,
<img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/> and <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/>, the latter three are known as the &#8216;parents&#8217; of
<img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/> and <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/> is called their &#8216;child&#8217;. Similarly, the parents of
<img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/> are <img class="math" src="_images/math/8e0b9f92fa1d1f3b8b4c42698703c470dad70615.png" alt="t_l"/> and <img class="math" src="_images/math/cdb0b5d73c0bcf07ab79e7d5929b5f6c2906c75c.png" alt="t_h"/>, and <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/> is the child of
<img class="math" src="_images/math/8e0b9f92fa1d1f3b8b4c42698703c470dad70615.png" alt="t_l"/> and <img class="math" src="_images/math/cdb0b5d73c0bcf07ab79e7d5929b5f6c2906c75c.png" alt="t_h"/>.</p>
</div>
<div class="section" id="two-types-of-variables">
<h2>3.2. Two types of variables<a class="headerlink" href="#two-types-of-variables" title="Permalink to this headline">¶</a></h2>
<p>At the model-specification stage (before the data are observed), <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/>,
<img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>, <img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/>, <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> and <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/> are all random variables. Bayesian
&#8216;random&#8217; variables have not necessarily arisen from a physical random process.
The Bayesian interpretation of probability is <em>epistemic</em>, meaning random
variable <img class="math" src="_images/math/26eeb5258ca5099acf8fe96b2a1049c48c89a5e6.png" alt="x"/>&#8216;s probability distribution <img class="math" src="_images/math/2751d79d3bbfb34440d68c685fe6ba7414951749.png" alt="p(x)"/> represents our
knowledge and uncertainty about <img class="math" src="_images/math/26eeb5258ca5099acf8fe96b2a1049c48c89a5e6.png" alt="x"/>&#8216;s value. Candidate values of <img class="math" src="_images/math/26eeb5258ca5099acf8fe96b2a1049c48c89a5e6.png" alt="x"/>
for which <img class="math" src="_images/math/2751d79d3bbfb34440d68c685fe6ba7414951749.png" alt="p(x)"/> is high are relatively more probable, given what we know.
Random variables are represented in PyMC by the classes <tt class="docutils literal"><span class="pre">Stochastic</span></tt> and
<tt class="docutils literal"><span class="pre">Deterministic</span></tt>.</p>
<p>The only <tt class="docutils literal"><span class="pre">Deterministic</span></tt> in the model is <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/>. If we knew the values of
<img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/>&#8216;s parents (<img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>, <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/> and <img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/>), we could compute the
value of <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> exactly. A <tt class="docutils literal"><span class="pre">Deterministic</span></tt> like <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> is defined by a
mathematical function that returns its value given values for its parents. The
nomenclature is a bit confusing, because these objects usually represent random
variables; since the parents of <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> are random, <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> is random also.
A more descriptive (though more awkward) name for this class would be
<tt class="docutils literal"><span class="pre">DeterminedByValuesOfParents</span></tt>.</p>
<p>On the other hand, even if the values of the parents of variables <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>,
<img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/> (before observing the data), <img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/> or <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/> were known, we
would still be uncertain of their values. These variables are characterized by
probability distributions that express how plausible their candidate values are,
given values for their parents. The <tt class="docutils literal"><span class="pre">Stochastic</span></tt> class represents these
variables. A more descriptive name for these objects might be
<tt class="docutils literal"><span class="pre">RandomEvenGivenValuesOfParents</span></tt>.</p>
<p>We can represent model <a href="#equation-disastermodel">(1)</a> in a file called
<tt class="xref docutils literal"><span class="pre">DisasterModel.py</span></tt> as follows. First, we import the PyMC and NumPy
namespaces:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">from</span> <span class="nn">pymc</span> <span class="k">import</span> <span class="n">DiscreteUniform</span><span class="p">,</span> <span class="n">Exponential</span><span class="p">,</span> <span class="n">deterministic</span><span class="p">,</span> <span class="n">Poisson</span><span class="p">,</span> <span class="n">Uniform</span>
<span class="k">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
<p>Notice that from <tt class="docutils literal"><span class="pre">pymc</span></tt> we have only imported a select few objects that are
needed for this particular model, whereas the entire <tt class="docutils literal"><span class="pre">numpy</span></tt> namespace has
been imported, and conveniently given a shorter name. Objects from NumPy are
subsequently accessed by prefixing <tt class="docutils literal"><span class="pre">np.</span></tt> to the name. Either approach is
acceptable.</p>
<p>Next, we enter the actual data values into an array:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">disasters_array</span> <span class="o">=</span>   <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">5</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">6</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">6</span><span class="p">,</span>
                   <span class="mf">3</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">5</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">5</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">5</span><span class="p">,</span> <span class="mf">5</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">5</span><span class="p">,</span>
                   <span class="mf">2</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span>
                   <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span>
                   <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span>
                   <span class="mf">3</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span>
                   <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">])</span>
</pre></div>
</div>
<p>Next, we create the switchpoint variable <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">s</span> <span class="o">=</span> <span class="n">DiscreteUniform</span><span class="p">(</span><span class="s">&#39;s&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">0</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">110</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="s">&#39;Switchpoint[year]&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><tt class="docutils literal"><span class="pre">DiscreteUniform</span></tt> is a subclass of <tt class="docutils literal"><span class="pre">Stochastic</span></tt> that represents uniformly-
distributed discrete variables. Use of this distribution suggests that we have
no preference <em>a priori</em> regarding the location of the switchpoint; all values
are equally likely. Now we create the exponentially-distributed variables
<img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/> and <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/> for the early and late Poisson rates, respectively:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">e</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="s">&#39;e&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1</span><span class="p">)</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="s">&#39;l&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Next, we define the variable <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/>, which selects the early rate <img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/>
for times before <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/> and the late rate <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/> for times after
<img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>. We create <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> using the <tt class="docutils literal"><span class="pre">deterministic</span></tt> decorator, which
converts the ordinary Python function <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> into a <tt class="docutils literal"><span class="pre">Deterministic</span></tt> object.</p>
<div class="highlight-python"><pre>@deterministic(plot=False)
def r(s=s, e=e, l=l):
     """ Concatenate Poisson means """
    out = np.empty(len(disasters_array))
    out[:s] = e
    out[s:] = l
    return out</pre>
</div>
<p>The last step is to define the number of disasters <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/>. This is a
stochastic variable, but unlike <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>, <img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/> and <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/> we have
observed its value. To express this, we set the argument <tt class="docutils literal"><span class="pre">observed</span></tt> to
<tt class="xref docutils literal"><span class="pre">True</span></tt> (it is set to <tt class="xref docutils literal"><span class="pre">False</span></tt> by default). This tells PyMC that this object&#8217;s
value should not be changed:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">D</span> <span class="o">=</span> <span class="n">Poisson</span><span class="p">(</span><span class="s">&#39;D&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">r</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">disasters_array</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="why-are-data-and-unknown-variables-represented-by-the-same-object">
<h3>3.2.1. Why are data and unknown variables represented by the same object?<a class="headerlink" href="#why-are-data-and-unknown-variables-represented-by-the-same-object" title="Permalink to this headline">¶</a></h3>
<p>Since it is represented by a <tt class="docutils literal"><span class="pre">Stochastic</span></tt> object, <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/> is defined by its
dependence on its parent <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> even though its value is fixed. This isn&#8217;t
just a quirk of PyMC&#8217;s syntax; Bayesian hierarchical notation itself makes no
distinction between random variables and data. The reason is simple: to use
Bayes&#8217; theorem to compute the posterior <img class="math" src="_images/math/d2e9091fdd655edf80f1422b75266ab87bf8e2d3.png" alt="p(e,s,l|D)"/> of model
<a href="#equation-disastermodel">(1)</a>, we require the likelihood <img class="math" src="_images/math/fc7d1cac324388611b3aa9dae3fa0485003565d3.png" alt="p(D|e,s,l)=p(D|r)"/>. Even
though <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/>&#8216;s value is known and fixed, we need to formally assign it a
probability distribution as if it were a random variable. Remember, the
likelihood and the probability function are essentially the same, except that
the former is regarded as a function of the parameters and the latter as a
function of the data.</p>
<p>This point can be counterintuitive at first, as many peoples&#8217; instinct is to
regard data as fixed a priori and unknown variables as dependent on the data.
One way to understand this is to think of statistical models like
(<a href="#equation-disastermodel">(1)</a>) as predictive models for data, or as models of the
processes that gave rise to data. Before observing the value of <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/>, we
could have sampled from its prior predictive distribution <img class="math" src="_images/math/624c26c4c330b2155139292fb775b25a3b1863b2.png" alt="p(D)"/> (<em>i.e.</em>
the marginal distribution of the data) as follows:</p>
<ol class="arabic simple">
<li>Sample <img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/>, <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/> and <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/> from their priors.</li>
<li>Sample <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/> conditional on these values.</li>
</ol>
<p>Even after we observe the value of <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/>, we need to use this process model
to make inferences about <img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/>, <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/> and <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/> because its the only
information we have about how the variables are related.</p>
</div>
</div>
<div class="section" id="parents-and-children">
<h2>3.3. Parents and children<a class="headerlink" href="#parents-and-children" title="Permalink to this headline">¶</a></h2>
<p>We have above created a PyMC probability model, which is simply a linked
collection of variables. To see the nature of the links, import or run
<tt class="docutils literal"><span class="pre">DisasterModel.py</span></tt> and examine <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>&#8216;s <tt class="docutils literal"><span class="pre">parents</span></tt> attribute from the
Python prompt:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">parents</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">{</span><span class="s">&#39;lower&#39;</span><span class="p">:</span> <span class="mf">0</span><span class="p">,</span> <span class="s">&#39;upper&#39;</span><span class="p">:</span> <span class="mf">110</span><span class="p">}</span>
</pre></div>
</div>
<p>The <tt class="docutils literal"><span class="pre">parents</span></tt> dictionary shows us the distributional parameters of <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>,
which are constants. Now let&#8217;s examinine <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/>&#8216;s parents:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">D</span><span class="o">.</span><span class="n">parents</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">{</span><span class="s">&#39;mu&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">pymc</span><span class="o">.</span><span class="n">PyMCObjects</span><span class="o">.</span><span class="n">Deterministic</span> <span class="s">&#39;r&#39;</span> <span class="n">at</span> <span class="mf">0</span><span class="n">x3e51a70</span><span class="o">&gt;</span><span class="p">}</span>
</pre></div>
</div>
<p>We are using <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> as a distributional parameter of <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/> (<em>i.e.</em>
<img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> is <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/>&#8216;s parent). <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/> internally labels <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> as
<tt class="docutils literal"><span class="pre">mu</span></tt>, meaning <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> plays the role of the rate parameter in <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/>&#8216;s
Poisson distribution. Now examine <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/>&#8216;s <tt class="docutils literal"><span class="pre">children</span></tt> attribute:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">r</span><span class="o">.</span><span class="n">children</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">set</span><span class="p">([</span><span class="o">&lt;</span><span class="n">pymc</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Poisson</span> <span class="s">&#39;D&#39;</span> <span class="n">at</span> <span class="mf">0</span><span class="n">x3e51290</span><span class="o">&gt;</span><span class="p">])</span>
</pre></div>
</div>
<p>Because <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/> considers <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> its parent, <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> considers <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/>
its child. Unlike <tt class="docutils literal"><span class="pre">parents</span></tt>, <tt class="docutils literal"><span class="pre">children</span></tt> is a set (an unordered collection of
objects); variables do not associate their children with any particular
distributional role. Try examining the <tt class="docutils literal"><span class="pre">parents</span></tt> and <tt class="docutils literal"><span class="pre">children</span></tt> attributes
of the other parameters in the model.</p>
<p>The following &#8216;directed acyclic graph&#8217; is a visualization of the parent-child
relationships in the model. Unobserved stochastic variables <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>, <img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/>
and <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/> are open ellipses, observed stochastic variable <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/> is a
filled ellipse and deterministic variable <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> is a triangle. Arrows point
from parent to child and display the label that the child assigns to the parent.
See section <a class="reference external" href="modelbuilding.html#graphical"><em>Graphing models</em></a> for more details.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Missing image.</p>
</div>
<div align="center" class="figure">
<a class="reference external image-reference" href="DisasterModel2.pdf"><img alt="Disasters time series." src="DisasterModel2.pdf" /></a>
</div>
</div>
<div class="section" id="variables-values-and-log-probabilities">
<h2>3.4. Variables&#8217; values and log-probabilities<a class="headerlink" href="#variables-values-and-log-probabilities" title="Permalink to this headline">¶</a></h2>
<p>All PyMC variables have an attribute called <tt class="docutils literal"><span class="pre">value</span></tt> that stores the current
value of that variable. Try examining <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/>&#8216;s value, and you&#8217;ll see the
initial value we provided for it:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">D</span><span class="o">.</span><span class="n">value</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="go">array([4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6, 3, 3, 5, 4, 5, 3, 1,</span>
<span class="go">       4, 4, 1, 5, 5, 3, 4, 2, 5, 2, 2, 3, 4, 2, 1, 3, 2, 2, 1, 1, 1, 1, 3,</span>
<span class="go">       0, 0, 1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1, 0, 1, 0, 1, 0,</span>
<span class="go">       0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2, 3, 3, 1, 1, 2, 1, 1, 1, 1, 2, 4, 2,</span>
<span class="go">       0, 0, 1, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])</span>
</pre></div>
</div>
<p>If you check <img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/>&#8216;s, <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>&#8216;s and <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/>&#8216;s values, you&#8217;ll see random
initial values generated by PyMC:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">value</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">44</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">value</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">0.33464706250079584</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span><span class="o">.</span><span class="n">value</span>
<span class="gp">&gt;&gt;&gt; </span><span class="mf">2.6491936762267811</span>
</pre></div>
</div>
<p>Of course, since these are <tt class="docutils literal"><span class="pre">Stochastic</span></tt> elements, your values will be
different than these. If you check <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/>&#8216;s value, you&#8217;ll see an array whose
first <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/> elements are <img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/> (here 0.33464706), and whose remaining
elements are <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/> (here 2.64919368):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">r</span><span class="o">.</span><span class="n">value</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="go">array([ 0.33464706,  0.33464706,  0.33464706,  0.33464706,  0.33464706,</span>
<span class="go">        0.33464706,  0.33464706,  0.33464706,  0.33464706,  0.33464706,</span>
<span class="go">        0.33464706,  0.33464706,  0.33464706,  0.33464706,  0.33464706,</span>
<span class="go">        0.33464706,  0.33464706,  0.33464706,  0.33464706,  0.33464706,</span>
<span class="go">        0.33464706,  0.33464706,  0.33464706,  0.33464706,  0.33464706,</span>
<span class="go">        0.33464706,  0.33464706,  0.33464706,  0.33464706,  0.33464706,</span>
<span class="go">        0.33464706,  0.33464706,  0.33464706,  0.33464706,  0.33464706,</span>
<span class="go">        0.33464706,  0.33464706,  0.33464706,  0.33464706,  0.33464706,</span>
<span class="go">        0.33464706,  0.33464706,  0.33464706,  0.33464706,  2.64919368,</span>
<span class="go">        2.64919368,  2.64919368,  2.64919368,  2.64919368,  2.64919368,</span>
<span class="go">        2.64919368,  2.64919368,  2.64919368,  2.64919368,  2.64919368,</span>
<span class="go">        2.64919368,  2.64919368,  2.64919368,  2.64919368,  2.64919368,</span>
<span class="go">        2.64919368,  2.64919368,  2.64919368,  2.64919368,  2.64919368,</span>
<span class="go">        2.64919368,  2.64919368,  2.64919368,  2.64919368,  2.64919368,</span>
<span class="go">        2.64919368,  2.64919368,  2.64919368,  2.64919368,  2.64919368,</span>
<span class="go">        2.64919368,  2.64919368,  2.64919368,  2.64919368,  2.64919368,</span>
<span class="go">        2.64919368,  2.64919368,  2.64919368,  2.64919368,  2.64919368,</span>
<span class="go">        2.64919368,  2.64919368,  2.64919368,  2.64919368,  2.64919368,</span>
<span class="go">        2.64919368,  2.64919368,  2.64919368,  2.64919368,  2.64919368,</span>
<span class="go">        2.64919368,  2.64919368,  2.64919368,  2.64919368,  2.64919368,</span>
<span class="go">        2.64919368,  2.64919368,  2.64919368,  2.64919368,  2.64919368,</span>
<span class="go">        2.64919368,  2.64919368,  2.64919368,  2.64919368,  2.64919368])</span>
</pre></div>
</div>
<p>To compute its value, <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> calls the funtion we used to create it, passing
in the values of its parents.</p>
<p><tt class="docutils literal"><span class="pre">Stochastic</span></tt> objects can evaluate their probability mass or density functions
at their current values given the values of their parents. The logarithm of a
stochastic object&#8217;s probability mass or density can be accessed via the <tt class="docutils literal"><span class="pre">logp</span></tt>
attribute. For vector-valued variables like <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/>, the <tt class="docutils literal"><span class="pre">logp</span></tt> attribute
returns the sum of the logarithms of the joint probability or density of all
elements of the value. Try examining <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>&#8216;s and <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/>&#8216;s log-
probabilities and <img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/>&#8216;s and <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/>&#8216;s log-densities:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">logp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">4.7095302013123339</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span><span class="o">.</span><span class="n">logp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">1080.5149888046033</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">logp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">0.33464706250079584</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span><span class="o">.</span><span class="n">logp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">-</span><span class="mf">2.6491936762267811</span>
</pre></div>
</div>
<p><tt class="docutils literal"><span class="pre">Stochastic</span></tt> objects need to call an internal function to compute their
<tt class="docutils literal"><span class="pre">logp</span></tt> attributes, as <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> needed to call an internal function to compute
its value. Just as we created <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> by decorating a function that computes
its value, it&#8217;s possible to create custom <tt class="docutils literal"><span class="pre">Stochastic</span></tt> objects by decorating
functions that compute their log-probabilities or densities (see chapter
<a class="reference external" href="modelbuilding.html#chap-modelbuilding"><em>Building models</em></a>). Users are thus not limited to the set of of
statistical distributions provided by PyMC.</p>
<div class="section" id="using-variables-as-parents-of-other-variables">
<h3>3.4.1. Using Variables as parents of other Variables<a class="headerlink" href="#using-variables-as-parents-of-other-variables" title="Permalink to this headline">¶</a></h3>
<p>Let&#8217;s take a closer look at our definition of <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="nd">@deterministic</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">r</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">s</span><span class="p">,</span> <span class="n">e</span><span class="o">=</span><span class="n">e</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">l</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Concatenate Poisson means &quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">disasters_array</span><span class="p">))</span>
    <span class="n">out</span><span class="p">[:</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">e</span>
    <span class="n">out</span><span class="p">[</span><span class="n">s</span><span class="p">:]</span> <span class="o">=</span> <span class="n">l</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>The arguments <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>, <img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/> and <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/> are <tt class="docutils literal"><span class="pre">Stochastic</span></tt> objects, not
numbers. Why aren&#8217;t errors raised when we attempt to slice array <tt class="docutils literal"><span class="pre">out</span></tt> up to a
<tt class="docutils literal"><span class="pre">Stochastic</span></tt> object?</p>
<p>Whenever a variable is used as a parent for a child variable, PyMC replaces it
with its <tt class="docutils literal"><span class="pre">value</span></tt> attribute when the child&#8217;s value or log-probability is
computed. When <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/>&#8216;s value is recomputed, <tt class="docutils literal"><span class="pre">s.value</span></tt> is passed to the
function as argument <tt class="docutils literal"><span class="pre">s</span></tt>. To see the values of the parents of <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> all
together, look at <tt class="docutils literal"><span class="pre">r.parents.value</span></tt>.</p>
</div>
</div>
<div class="section" id="fitting-the-model-with-mcmc">
<h2>3.5. Fitting the model with MCMC<a class="headerlink" href="#fitting-the-model-with-mcmc" title="Permalink to this headline">¶</a></h2>
<p>PyMC provides several objects that fit probability models (linked collections of
variables) like ours. The primary such object, <tt class="docutils literal"><span class="pre">MCMC</span></tt>, fits models with the
Markov chain Monte Carlo algorithm. See appendix <em class="xref">chap:mcmc</em> for an
introduction to the algorithm itself. To create an <tt class="docutils literal"><span class="pre">MCMC</span></tt> object to handle our
model, import <tt class="xref docutils literal"><span class="pre">DisasterModel.py</span></tt> and use it as an argument for <tt class="docutils literal"><span class="pre">MCMC</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">import</span> <span class="nn">DisasterModel</span>
<span class="k">from</span> <span class="nn">pymc</span> <span class="k">import</span> <span class="n">MCMC</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">DisasterModel</span><span class="p">)</span>
</pre></div>
</div>
<p>In this case <tt class="docutils literal"><span class="pre">M</span></tt> will expose variables <tt class="docutils literal"><span class="pre">s</span></tt>, <tt class="docutils literal"><span class="pre">e</span></tt>, <tt class="docutils literal"><span class="pre">l</span></tt>, <tt class="docutils literal"><span class="pre">r</span></tt> and <tt class="docutils literal"><span class="pre">D</span></tt> as
attributes; that is, <tt class="docutils literal"><span class="pre">M.s</span></tt> will be the same object as <tt class="docutils literal"><span class="pre">DisasterModel.s</span></tt>.</p>
<p>To run the sampler, call the MCMC object&#8217;s <tt class="docutils literal"><span class="pre">isample()</span></tt> (or <tt class="docutils literal"><span class="pre">sample()</span></tt>)
method with arguments for the number of iterations, burn-in length, and thinning
interval (if desired):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">M</span><span class="o">.</span><span class="n">isample</span><span class="p">(</span><span class="nb">iter</span><span class="o">=</span><span class="mf">10000</span><span class="p">,</span> <span class="n">burn</span><span class="o">=</span><span class="mf">1000</span><span class="p">,</span> <span class="n">thin</span><span class="o">=</span><span class="mf">10</span><span class="p">)</span>
</pre></div>
</div>
<p>After a few seconds, you should see that sampling has finished normally. The
model has been fitted.</p>
<div class="section" id="what-does-it-mean-to-fit-a-model">
<h3>3.5.1. What does it mean to fit a model?<a class="headerlink" href="#what-does-it-mean-to-fit-a-model" title="Permalink to this headline">¶</a></h3>
<p>&#8216;Fitting&#8217; a model means characterizing its posterior distribution somehow. In
this case, we are trying to represent the posterior <img class="math" src="_images/math/69bb80927b6a922910f5bb77cc95a872d6a12eff.png" alt="p(s,e,l|D)"/> by a set
of joint samples from it. To produce these samples, the MCMC sampler randomly
updates the values of <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>, <img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/> and <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/> according to the
Metropolis-Hastings algorithm ([Gelman et al., 2004]_) for <tt class="docutils literal"><span class="pre">iter</span></tt>  iterations.</p>
<p>After a sufficiently large number of iterations, the current values of
<img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>, <img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/> and <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/> can be considered a sample from the
posterior. PyMC assumes that the <tt class="docutils literal"><span class="pre">burn</span></tt> parameter specifies a &#8216;sufficiently
large&#8217; number of iterations for convergence of the algorithm, so it is up to the
user to verify that this is the case (see chapter <em class="xref">chap:modelchecking</em>).
Consecutive values sampled from <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>, <img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/> and <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/> are
necessarily dependent on the previous sample, since it is a Markov chain.
However, MCMC often results in strong autocorrelation among samples that can
result in imprecise posterior inference. To circumvent this, it is often
effective to thin the sample by only retaining every <img class="math" src="_images/math/3c8b4324ecdf8b162762510240c9462a5aab903f.png" alt="k`th sample, where
:math:`k"/> is an integer value. This thinning interval is passed to the sampler
via the <tt class="docutils literal"><span class="pre">thin</span></tt> argument.</p>
<p>If you are not sure ahead of time what values to choose for the <tt class="docutils literal"><span class="pre">burn</span></tt> and
<tt class="docutils literal"><span class="pre">thin</span></tt> parameters, you may want to retain all the MCMC samples, that is to set
<tt class="docutils literal"><span class="pre">burn=0</span></tt> and <tt class="docutils literal"><span class="pre">thin=1</span></tt>, and then discard the &#8216;burnin period&#8217; and thin the
samples after examining the traces (the series of samples). See [Gelman et al., 2004]_ for
general guidance.</p>
</div>
<div class="section" id="accessing-the-samples">
<h3>3.5.2. Accessing the samples<a class="headerlink" href="#accessing-the-samples" title="Permalink to this headline">¶</a></h3>
<p>The output of the MCMC algorithm is a &#8216;trace&#8217;, the sequence of retained samples
for each variable in the model. These traces can be accessed using the
<tt class="docutils literal"><span class="pre">trace(name,</span> <span class="pre">chain=-1)</span></tt> method. For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">M</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="s">&#39;s&#39;</span><span class="p">)[:]</span>
<span class="go">array([41, 40, 40, ..., 43, 44, 44])</span>
</pre></div>
</div>
<p>The trace slice <tt class="docutils literal"><span class="pre">[start:stop:step]</span></tt> works just like the NumPy array slice. By
default, the returned trace array contains the samples from the last call to
<tt class="docutils literal"><span class="pre">sample</span></tt>, that is, <tt class="docutils literal"><span class="pre">chain=-1</span></tt>, but the trace from previous sampling runs can
be retrieved by specifying the correspondent chain index. To return the trace
from all chains, simply use <tt class="docutils literal"><span class="pre">chain=None</span></tt>. <a class="footnote-reference" href="#id3" id="id2">[1]</a></p>
</div>
<div class="section" id="sampling-output">
<h3>3.5.3. Sampling output<a class="headerlink" href="#sampling-output" title="Permalink to this headline">¶</a></h3>
<p>You can examine the marginal posterior of any variable by plotting a histogram
of its trace:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">from</span> <span class="nn">pylab</span> <span class="k">import</span> <span class="n">hist</span><span class="p">,</span> <span class="n">show</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hist</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="s">&#39;l&#39;</span><span class="p">)[:])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="go">(array([   8,   52,  565, 1624, 2563, 2105, 1292,  488,  258,   45]),</span>
<span class="go"> array([ 0.52721865,  0.60788251,  0.68854637,  0.76921023,  0.84987409,</span>
<span class="go">        0.93053795,  1.01120181,  1.09186567,  1.17252953,  1.25319339]),</span>
<span class="go"> &lt;a list of 10 Patch objects&gt;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>You should see something like this:</p>
<p>PyMC has its own plotting functionality, via the optional <tt class="xref docutils literal"><span class="pre">matplotlib</span></tt>
module as noted in the installation notes. The <tt class="xref docutils literal"><span class="pre">Matplot</span></tt> module includes a
<tt class="docutils literal"><span class="pre">plot</span></tt> function that takes the model (or a single parameter) as an argument:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">from</span> <span class="nn">pymc.Matplot</span> <span class="k">import</span> <span class="n">plot</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plot</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
</pre></div>
</div>
<p>For each variable in the model, <tt class="docutils literal"><span class="pre">plot</span></tt> generates a composite figure, such as
this one for the switchpoint in the disasters model:</p>
<p>The left-hand pane of this figure shows the temporal series of the samples from
<img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>, while the right-hand pane shows a histogram of the trace. The trace
is useful for evaluating and diagnosing the algorithm&#8217;s performance (see
[Gelman et al., 2004]_), while the histogram is useful for visualizing the posterior.</p>
<p>For a non-graphical summary of the posterior, simply call <tt class="docutils literal"><span class="pre">M.stats()</span></tt>.</p>
</div>
<div class="section" id="imputation-of-missing-data">
<h3>3.5.4. Imputation of Missing Data<a class="headerlink" href="#imputation-of-missing-data" title="Permalink to this headline">¶</a></h3>
<p>As with most &#8220;textbook examples&#8221;, the models we have examined so far assume that
the associated data are complete. That is, there are no missing values
corresponding to any observations in the dataset. However, many real-world
datasets contain one or more missing values, usually due to some logistical
problem during the data collection process. The easiest way of dealing with
observations that contain missing values is simply to exclude them from the
analysis. However, this results in loss of information if an excluded
observation contains valid values for other quantities. An alternative is to
impute the missing values, based on information in the rest of the model.</p>
<p>For example, consider a survey dataset for some wildlife species:</p>
<table border="1" class="docutils">
<colgroup>
<col width="18%" />
<col width="14%" />
<col width="29%" />
<col width="39%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Count</th>
<th class="head">Site</th>
<th class="head">Observer</th>
<th class="head">Temperature</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>15</td>
<td>1</td>
<td>1</td>
<td>15</td>
</tr>
<tr><td>10</td>
<td>1</td>
<td>2</td>
<td>NA</td>
</tr>
<tr><td>6</td>
<td>1</td>
<td>1</td>
<td>11</td>
</tr>
</tbody>
</table>
<p>Each row contains the number of individuals seen during the survey, along with
three covariates: the site on which the survey was conducted, the observer that
collected the data, and the temperature during the survey. If we are interested
in modelling, say, population size as a function of the count and the associated
covariates, it is difficult to accommodate the second observation because the
temperature is missing (perhaps the thermometer was broken that day). Ignoring
this observation will allow us to fit the model, but it wastes information that
is contained in the other covariates.</p>
<p>In a Bayesian modelling framework, missing data are accommodated simply by
treating them as unknown model parameters. Values for the missing data
<img class="math" src="_images/math/d3996d8ffdbe9dc1bc1d53445a8fd4082c187d86.png" alt="\tilde{y}"/> are estimated naturally, using the posterior predictive
distribution:</p>
<div class="math">
<p><img src="_images/math/cad45692fc0a7d9776d053ea25d278bcdbb52850.png" alt="p(\tilde{y}|y) = \int p(\tilde{y}|\theta) f(\theta|y) d\theta" />
</div></p><p>This describes additional data <img class="math" src="_images/math/d3996d8ffdbe9dc1bc1d53445a8fd4082c187d86.png" alt="\tilde{y}"/>, which may either be considered
unobserved data or potential future observations. We can use the posterior
predictive distribution to model the likely values of missing data.</p>
<p>Consider the coal mining disasters data introduced previously. Assume that two
years of data are missing from the time series; we indicate this in the data
array by the use of an arbitrary placeholder value, -999.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">5</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">6</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">6</span><span class="p">,</span>
<span class="mf">3</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">5</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">5</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">5</span><span class="p">,</span> <span class="mf">5</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">5</span><span class="p">,</span>
<span class="mf">2</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">999</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span>
<span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span>
<span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span>
<span class="mf">3</span><span class="p">,</span> <span class="mf">3</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">999</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span> <span class="mf">2</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">4</span><span class="p">,</span>
<span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">,</span> <span class="mf">0</span><span class="p">,</span> <span class="mf">1</span><span class="p">])</span>
</pre></div>
</div>
<p>To estimate these values in PyMC, we generate a masked array. These are
specialised NumPy arrays that contain a matching True or False value for each
element to indicate if that value should be excluded from any computation.
Masked arrays can be generated using NumPy&#8217;s <tt class="docutils literal"><span class="pre">ma.masked_equal</span></tt> function:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">masked_data</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">masked_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="mf">999</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">masked_data</span>
<span class="go">masked_array(data = [4 5 4 0 1 4 3 4 0 6 3 3 4 0 2 6 3 3 5 4 5 3 1 4 4 1 5 5 3</span>
<span class="go"> 4 2 5 2 2 3 4 2 1 3 -- 2 1 1 1 1 3 0 0 1 0 1 1 0 0 3 1 0 3 2 2 0 1 1 1 0 1 0</span>
<span class="go"> 1 0 0 0 2 1 0 0 0 1 1 0 2 3 3 1 -- 2 1 1 1 1 2 4 2 0 0 1 4 0 0 0 1 0 0 0 0 0 1</span>
<span class="go"> 0 0 1 0 1],</span>
<span class="go"> mask = [False False False False False False False False False False False False</span>
<span class="go"> False False False False False False False False False False False False</span>
<span class="go"> False False False False False False False False False False False False</span>
<span class="go"> False False False  True False False False False False False False False</span>
<span class="go"> False False False False False False False False False False False False</span>
<span class="go"> False False False False False False False False False False False False</span>
<span class="go"> False False False False False False False False False False False  True</span>
<span class="go"> False False False False False False False False False False False False</span>
<span class="go"> False False False False False False False False False False False False</span>
<span class="go"> False False False],</span>
<span class="go">      fill_value=999999)</span>
</pre></div>
</div>
<p>This masked array, in turn, can then be passed to PyMC&#8217;s own <tt class="docutils literal"><span class="pre">ImputeMissing</span></tt>
function, which replaces the missing values with Stochastic variables of the
desired type. For the coal mining disasters problem, recall that disaster events
were modelled as Poisson variates:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="n">ImputeMissing</span><span class="p">(</span><span class="s">&#39;D&#39;</span><span class="p">,</span> <span class="n">Poisson</span><span class="p">,</span> <span class="n">masked_data</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">r</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span>
<span class="go">[&lt;pymc.distributions.Poisson &#39;D[0]&#39; at 0x4ba42d0&gt;,</span>
<span class="go"> &lt;pymc.distributions.Poisson &#39;D[1]&#39; at 0x4ba4330&gt;,</span>
<span class="go"> &lt;pymc.distributions.Poisson &#39;D[2]&#39; at 0x4ba44d0&gt;,</span>
<span class="go"> &lt;pymc.distributions.Poisson &#39;D[3]&#39; at 0x4ba45f0&gt;,</span>
<span class="gp">...</span>
<span class="go"> &lt;pymc.distributions.Poisson &#39;D[110]&#39; at 0x4ba46d0&gt;]</span>
</pre></div>
</div>
<p>Here <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/> is an array of means for each year of data, allocated according
to the location of the switchpoint. Each element in <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/> is a Poisson
Stochastic, irrespective of whether the observation was missing or not. The
difference is that actual observations are data Stochastics (<tt class="docutils literal"><span class="pre">observed=True</span></tt>),
while the missing values are non-data Stochastics. The latter are considered
unknown, rather than fixed, and therefore estimated by the MCMC algorithm, just
as unknown model parameters.</p>
<p>The entire model looks very similar to the original model:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># Switchpoint</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">DiscreteUniform</span><span class="p">(</span><span class="s">&#39;s&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">0</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">110</span><span class="p">)</span>
<span class="c"># Early mean</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="s">&#39;e&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1</span><span class="p">)</span>
<span class="c"># Late mean</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="s">&#39;l&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1</span><span class="p">)</span>

<span class="nd">@deterministic</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">r</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">s</span><span class="p">,</span> <span class="n">e</span><span class="o">=</span><span class="n">e</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">l</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Allocate appropriate mean to time series&quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">disasters_array</span><span class="p">))</span>
    <span class="c"># Early mean prior to switchpoint</span>
    <span class="n">out</span><span class="p">[:</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">e</span>
    <span class="c"># Late mean following switchpoint</span>
    <span class="n">out</span><span class="p">[</span><span class="n">s</span><span class="p">:]</span> <span class="o">=</span> <span class="n">l</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="c"># Where the mask is true, the value is taken as missing.</span>
<span class="n">masked_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">masked_array</span><span class="p">(</span><span class="n">disasters_array</span><span class="p">,</span> <span class="n">disasters_mask</span><span class="p">)</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">ImputeMissing</span><span class="p">(</span><span class="s">&#39;D&#39;</span><span class="p">,</span> <span class="n">Poisson</span><span class="p">,</span> <span class="n">masked_data</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">r</span><span class="p">)</span>
</pre></div>
</div>
<p>The main limitation of this approach for imputation is performance. Because each
element in the data array is modelled by an individual Stochastic, rather than a
single Stochastic for the entire array, the number of nodes in the overall model
increases from 4 to 113. This significantly slows the rate of sampling, since
the model iterates over each node at every iteration.</p>
<div align="center" class="figure" id="fig-missing">
<a class="reference external image-reference" href="_images/missing.png"><img alt="Trace and posterior distribution figure." src="_images/missing.png" style="width: 700.0px; height: 420.0px;" /></a>
<p class="caption">Trace and posterior distribution of the second missing data point in the example.</p>
</div>
</div>
</div>
<div class="section" id="fine-tuning-the-mcmc-algorithm">
<h2>3.6. Fine-tuning the MCMC algorithm<a class="headerlink" href="#fine-tuning-the-mcmc-algorithm" title="Permalink to this headline">¶</a></h2>
<p>MCMC objects handle individual variables via <em>step methods</em>, which determine how
parameters are updated at each step of the MCMC algorithm. By default, step
methods are automatically assigned to variables by PyMC. To see which step
methods <img class="math" src="_images/math/5d1e4485dc90c450e8c76826516c1b2ccb8fce16.png" alt="M"/> is using, look at its <tt class="docutils literal"><span class="pre">step_method_dict</span></tt> attribute with
respect to each parameter:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">M</span><span class="o">.</span><span class="n">step_method_dict</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">&lt;</span><span class="n">pymc</span><span class="o">.</span><span class="n">StepMethods</span><span class="o">.</span><span class="n">DiscreteMetropolis</span> <span class="nb">object</span> <span class="n">at</span> <span class="mf">0</span><span class="n">x3e8cb50</span><span class="o">&gt;</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">M</span><span class="o">.</span><span class="n">step_method_dict</span><span class="p">[</span><span class="n">e</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">&lt;</span><span class="n">pymc</span><span class="o">.</span><span class="n">StepMethods</span><span class="o">.</span><span class="n">Metropolis</span> <span class="nb">object</span> <span class="n">at</span> <span class="mf">0</span><span class="n">x3e8cbb0</span><span class="o">&gt;</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">M</span><span class="o">.</span><span class="n">step_method_dict</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="o">&lt;</span><span class="n">pymc</span><span class="o">.</span><span class="n">StepMethods</span><span class="o">.</span><span class="n">Metropolis</span> <span class="nb">object</span> <span class="n">at</span> <span class="mf">0</span><span class="n">x3e8ccb0</span><span class="o">&gt;</span><span class="p">]</span>
</pre></div>
</div>
<p>The value of <tt class="docutils literal"><span class="pre">step_method_dict</span></tt> corresponding to a particular variable is a
list of the step methods <img class="math" src="_images/math/5d1e4485dc90c450e8c76826516c1b2ccb8fce16.png" alt="M"/> is using to handle that variable.</p>
<p>You can force <img class="math" src="_images/math/5d1e4485dc90c450e8c76826516c1b2ccb8fce16.png" alt="M"/> to use a particular step method by calling
<tt class="docutils literal"><span class="pre">M.use_step_method</span></tt> before telling it to sample. The following call will cause
<img class="math" src="_images/math/5d1e4485dc90c450e8c76826516c1b2ccb8fce16.png" alt="M"/> to handle <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/> with a standard <tt class="docutils literal"><span class="pre">Metropolis</span></tt> step method, but
with proposal standard deviation equal to <img class="math" src="_images/math/41c544263a265ff15498ee45f7392c5f86c6d151.png" alt="2"/>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">M</span><span class="o">.</span><span class="n">use_step_method</span><span class="p">(</span><span class="n">Metropolis</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">proposal_sd</span><span class="o">=</span><span class="mf">2.</span><span class="p">)</span>
</pre></div>
</div>
<p>Another step method class, <tt class="docutils literal"><span class="pre">AdaptiveMetropolis</span></tt>, is better at handling highly-
correlated variables. If your model mixes poorly, using <tt class="docutils literal"><span class="pre">AdaptiveMetropolis</span></tt>
is a sensible first thing to try.</p>
</div>
<div class="section" id="beyond-the-basics">
<h2>3.7. Beyond the basics<a class="headerlink" href="#beyond-the-basics" title="Permalink to this headline">¶</a></h2>
<p>That was a brief introduction to basic PyMC usage. Many more topics are covered
in the subsequent sections, including:</p>
<ul class="simple">
<li>Class <tt class="docutils literal"><span class="pre">Potential</span></tt>, another building block for probability models in addition
to <tt class="docutils literal"><span class="pre">Stochastic</span></tt> and <tt class="docutils literal"><span class="pre">Deterministic</span></tt></li>
<li>Normal approximations</li>
<li>Using custom probability distributions</li>
<li>Object architecture</li>
<li>Saving traces to the disk, or streaming them to the disk during sampling</li>
<li>Writing your own step methods and fitting algorithms.</li>
</ul>
<p>Also, be sure to check out the documentation for the Gaussian process extension,
which is available on the webpage.</p>
<p class="rubric">Footnotes</p>
<table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[1]</a></td><td>Note that the unknown variables <img class="math" src="_images/math/f37bba504894945c07a32f5496d74299a37aa51c.png" alt="s"/>, <img class="math" src="_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/>, <img class="math" src="_images/math/9b25f8e64b484493fda944d25cad453423041fe6.png" alt="l"/> and <img class="math" src="_images/math/b55ca7a0aa88ab7d58f4fc035317fdac39b17861.png" alt="r"/>
will all accrue samples, but <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/> will not because its value has been
observed and is not updated. Hence <img class="math" src="_images/math/9ffb448918db29f2a72f8f87f421b3b3cad18f95.png" alt="D"/> has no trace and calling
<tt class="docutils literal"><span class="pre">M.trace('D')[:]</span></tt> will raise an error.</td></tr>
</tbody>
</table>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="modelbuilding.html" title="4. Building models"
             >next</a> |</li>
        <li class="right" >
          <a href="INSTALL.html" title="2. Installation Instructions"
             >previous</a> |</li>
        <li><a href="index.html">pymc v2.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
      &copy; Copyright 2008, Chris Fonnesbeck, Anand Patil, David Huard.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 0.7.
    </div>
  </body>
</html>