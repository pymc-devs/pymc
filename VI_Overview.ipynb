{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c51c3c1a-553c-45e4-a92f-d75063187863",
   "metadata": {},
   "source": [
    "# Variational Inference overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0777ef6-dd90-4452-88af-69a53f0f7713",
   "metadata": {},
   "source": [
    "## Existing Variational Inference implementation\n",
    "\n",
    "The best way to get a sense for the current implementation is to walk backwards from how it's used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d5724fe-72db-4908-a464-46f7fac97309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33437d00-60e6-4505-8b8e-8ebe64473c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.normal(size=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894f9e31-90a1-4f13-b2bc-b75bdf996f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    d = pm.Data(\"data\", data)\n",
    "    batched_data = pm.Minibatch(d, batch_size=100)\n",
    "    x = pm.Normal(\"x\", 0., 1.)\n",
    "    y = pm.Normal(\"y\", x, total_size=len(data), observed=batched_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15bc2997-8974-4d61-88ce-9423b215f84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a695972a8ca3415f9f8dd118ae6288dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished [100%]: Average Loss = 144.77\n"
     ]
    }
   ],
   "source": [
    "with model:\n",
    "    idata = pm.fit(n=10_000, method=\"advi\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311e2f2-f264-4cb2-9287-21d6f5aad3e3",
   "metadata": {},
   "source": [
    "But what does fit do? It roughly dispatches on the method. So the above is roughly equalivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec3b637d-c6a2-46cb-99bc-87fc952bda3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43686ad598a649b09a88b84480985eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished [100%]: Average Loss = 143.83\n"
     ]
    }
   ],
   "source": [
    "with model:\n",
    "    advi = pm.ADVI()\n",
    "    idata = advi.fit(n=100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd2a63-b5da-41d3-a1d1-254934ad4923",
   "metadata": {},
   "source": [
    "But what is this `ADVI` object? Well, if you look at it's implementation with the documentation removed, you see it's a type of `KLqp`\n",
    "\n",
    "````python\n",
    "class ADVI(KLqp):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(MeanField(*args, **kwargs))\n",
    "````\n",
    "\n",
    "So what's a `Klqp`? Look at it's implementation with the documentation removed, you see it's an Inference object\n",
    "\n",
    "````python\n",
    "class KLqp(Inference):\n",
    "    def __init__(self, approx, beta=1.0):\n",
    "        super().__init__(KL, approx, None, beta=beta)\n",
    "````\n",
    "\n",
    "So what's an `Inference` object? Look at it's implementation with the documentation removed we finally get a sense for what are the main abstraction we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce19d0bd-8a6b-4877-a4ee-5ee1fa481da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapprox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "**Base class for Variational Inference**.\n",
       "\n",
       "Communicates Operator, Approximation and Test Function to build Objective Function\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "op : Operator class    #:class:`~pymc.variational.operators`\n",
       "approx : Approximation class or instance    #:class:`~pymc.variational.approximations`\n",
       "tf : TestFunction instance  #?\n",
       "model : Model\n",
       "    PyMC Model\n",
       "kwargs : kwargs passed to :class:`Operator` #:class:`~pymc.variational.operators`, optional\n",
       "\u001b[0;31mFile:\u001b[0m           ~/upstream/pymc/pymc/variational/inference.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     KLqp, ImplicitGradient"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pm.Inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73873c2f-11ed-43a3-b256-2e65816343b9",
   "metadata": {},
   "source": [
    "Now things are falling into place. The `Inference` class is the way we perform variational inference. This is where the actual fit machinery lives. It also highlights what we need to do variational inference. We need a `Model`, an `Operator`, and an `Approximation`. We already know for `ADVI`, that the `Operator` is `KL` and the `Approximation` is `MeanField`.\n",
    "\n",
    "But what do these things mean? And how are they combined to perform inference?\n",
    "\n",
    "Well the `__init__` method of `Inference` makes it where we can find our answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a77e663-8982-4a8a-bafc-290da5f45838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapprox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m Initialize self.  See help(type(self)) for accurate signature.\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapprox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapprox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/upstream/pymc/pymc/variational/inference.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pm.Inference.__init__??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0564cc40-2d42-476d-8fa4-91a063bff433",
   "metadata": {},
   "source": [
    "Alright, so let's go ahead and explore the operator `KL`\n",
    "\n",
    "````python\n",
    "class KL(Operator):\n",
    "    def __init__(self, approx, beta=1.0):\n",
    "        super().__init__(approx)\n",
    "        self.beta = pm.floatX(beta)\n",
    "\n",
    "    def apply(self, f):\n",
    "        return -self.datalogp_norm + self.beta * (self.logq_norm - self.varlogp_norm)\n",
    "````\n",
    "\n",
    "We see no `__call__` but we see a call to the `__init__` of `Operator`. For the `apply` method we see what looks like the ELBO. Let's for now inline for `ADVI` case and see what we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1752272d-b32d-4bea-9c3c-1e331b7fda9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymc.variational.opvi.ObjectiveFunction at 0x70ee59332810>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective = pm.operators.KL(pm.MeanField(model=model))(None)\n",
    "objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70a473-6ca3-471c-b051-9a3cee666bd6",
   "metadata": {},
   "source": [
    "So how'd that happen? Well if you look in the `Objective` class you see\n",
    "\n",
    "````python\n",
    "    objective_class = ObjectiveFunction\n",
    "\n",
    "    def __call__(self, f=None):\n",
    "        if self.has_test_function:\n",
    "            if f is None:\n",
    "                raise ParametrizationError(f\"Operator {self} requires TestFunction\")\n",
    "            else:\n",
    "                if not isinstance(f, TestFunction):\n",
    "                    f = TestFunction.from_function(f)\n",
    "        else:\n",
    "            if f is not None:\n",
    "                warnings.warn(f\"TestFunction for {self} is redundant and removed\", stacklevel=3)\n",
    "            else:\n",
    "                pass\n",
    "            f = TestFunction()\n",
    "        f.setup(self.approx)\n",
    "        return self.objective_class(self, f)\n",
    "````\n",
    "\n",
    "Which finally brings us to `ObjectiveFunction`\n",
    "\n",
    "This is the function that sets up the actual loss functions and does the updates on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "339f72bf-a40d-4f84-9161-99a20ad090cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectiveFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mobj_n_mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtf_n_mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mobj_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0madagrad_window\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x70ee648da480\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtest_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0madagrad_window\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x70ee648da480\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmore_obj_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmore_tf_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmore_updates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmore_replacements\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtotal_grad_norm_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompile_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfn_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Step function that should be called on each optimization step.\n",
       "\n",
       "Generally it solves the following problem:\n",
       "\n",
       ".. math::\n",
       "\n",
       "        \\mathbf{\\lambda^{\\*}} = \\inf_{\\lambda} \\sup_{\\theta} t(\\mathbb{E}_{\\lambda}[(O^{p,q}f_{\\theta})(z)])\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "obj_n_mc: `int`\n",
       "    Number of monte carlo samples used for approximation of objective gradients\n",
       "tf_n_mc: `int`\n",
       "    Number of monte carlo samples used for approximation of test function gradients\n",
       "obj_optimizer: function (grads, params) -> updates\n",
       "    Optimizer that is used for objective params\n",
       "test_optimizer: function (grads, params) -> updates\n",
       "    Optimizer that is used for test function params\n",
       "more_obj_params: `list`\n",
       "    Add custom params for objective optimizer\n",
       "more_tf_params: `list`\n",
       "    Add custom params for test function optimizer\n",
       "more_updates: `dict`\n",
       "    Add custom updates to resulting updates\n",
       "total_grad_norm_constraint: `float`\n",
       "    Bounds gradient norm, prevents exploding gradient problem\n",
       "score: `bool`\n",
       "    calculate loss on each step? Defaults to False for speed\n",
       "compile_kwargs: `dict`\n",
       "    Add kwargs to pytensor.function (e.g. `{'profile': True}`)\n",
       "fn_kwargs: dict\n",
       "    arbitrary kwargs passed to `pytensor.function`\n",
       "\n",
       "    .. warning:: `fn_kwargs` is deprecated and will be removed in future versions\n",
       "\n",
       "more_replacements: `dict`\n",
       "    Apply custom replacements before calculating gradients\n",
       "\n",
       "Returns\n",
       "-------\n",
       "`pytensor.function`\n",
       "\u001b[0;31mFile:\u001b[0m      ~/upstream/pymc/pymc/variational/opvi.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pm.opvi.ObjectiveFunction.step_function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b5504c-6a99-40eb-8b45-b485a8618ecd",
   "metadata": {},
   "source": [
    "## Proposed Improvements\n",
    "\n",
    "There is a lot to like here, but there is also a lot of indirection. Further, much of it isn't used for the `ADVI` case. This is all in service of `SVGD` and `ASVGD`\n",
    "\n",
    "Further, the `Inference` class has to be aware of too many of these details. Ideally the `Inference` should be reworked to only take in a step function. It could be re-named `Trainer` to match what's in PyTorch Lightning. I think forcing all `VI` through `OPVI` makes it more challenging to write and port new `VI` algorithms to `pymc`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e83647-ac36-4043-8bb5-8fcf5581ffa3",
   "metadata": {},
   "source": [
    "### PyTorch Lightning and Optax optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece80fe0-e332-444d-b1e3-761aca6a02e8",
   "metadata": {},
   "source": [
    "How would this look? One possibility is having each Variational Inference technique encapsulated into an object that takes a model and optimizer as inputs and provides a step function as a method.\n",
    "\n",
    "````python\n",
    "class ADVI(Inference):\n",
    "    def __init__(self, model=None, optimizers=None):\n",
    "        ...\n",
    "\n",
    "    def step(self, batch):\n",
    "        ...\n",
    "        return loss\n",
    "````\n",
    "\n",
    "This is then passed to a `Trainer` object for fitting\n",
    "\n",
    "````python\n",
    "with model:\n",
    "    trainer = Trainer(method=ADVI(), dataloader= ...)\n",
    "    trainer.fit(n=10_000)\n",
    "````\n",
    "\n",
    "Under this setup most of the optimization logic moves into the `__init__` and `step` methods. As for how those should happen. I think this can be handled separately. But something like optax might not be so bad. So we could end with code that resembles the below\n",
    "\n",
    "````python\n",
    "class ADVI(Inference):\n",
    "    def __init__(self, model=None, optimizers=None):\n",
    "        if model is None:\n",
    "            model = modelcontext(None)\n",
    "        if optimizers is None:\n",
    "            optimizers = [pm.opt.Adam(1e-3)]\n",
    "        self.optimizer = optimizers[0]\n",
    "        self.params = self.optimizer.init(model.basic_RVs)\n",
    "\n",
    "    def step(self, batch):\n",
    "        loss = self.loss_function(self.params, batch)\n",
    "        grads = grad(loss)\n",
    "        self.params = self.optimizer.update(grads, self.params)\n",
    "        return loss\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5572e2b-6a32-492d-9cb0-d003a14f490d",
   "metadata": {},
   "source": [
    "### Model and Guide programs\n",
    "\n",
    "Additionally it would be nice if we could easily suppose variational inference with guide programs ala pyro/numpyro\n",
    "\n",
    "The way this could look is we define both as `pymc` models and then pass them to a `SVI` method\n",
    "\n",
    "````python\n",
    "with pm.Model() as model:\n",
    "    data = pm.Data(\"data\", ...)\n",
    "    x = pm.Normal(\"x\", 0, 1)\n",
    "    y = pm.Normal(\"y\", x, 1, observed=data)\n",
    "\n",
    "with pm.Model() as guide:\n",
    "    mu = pt.tensor(\"mu\", param=True)\n",
    "    sd = pt.tensor(\"sd\", param=True)\n",
    "    x = pm.Normal(\"x\", mu, sd)\n",
    "\n",
    "\n",
    "with model:\n",
    "    trainer = Trainer(method=SVI(model, guide), dataloader= ...)\n",
    "    trainer.fit(n=10_000)\n",
    "````\n",
    "\n",
    "Naturally, `SVI` is a very general inference method, and in fact we can re-define `ADVI` in terms of it. Following the lead of pyro/numpyro we can have a guide generation\n",
    "\n",
    "````python\n",
    "with model:\n",
    "    guide = AutoGuide(model)\n",
    "    trainer = Trainer(method=SVI(model, guide), dataloader= ...)\n",
    "    trainer.fit(n=10_000)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f97c341-e9bb-4301-b452-d006d6408cec",
   "metadata": {},
   "source": [
    "### Reworking Minibatch\n",
    "\n",
    "Another small change we should consider is moving `pm.Minibatch` out of the model. Max already has a [proposal](https://github.com/pymc-devs/pymc/issues/7496) that I think can be adopted with only a few changes.\n",
    "\n",
    "I think where before we explicitly minibatch the data, instead we have dataloaders that stream in updates to the model.\n",
    "\n",
    "````python\n",
    "with pm.Model() as model:\n",
    "    data = pm.Data(\"data\", None)\n",
    "    x = pm.Normal(\"x\", 0, 1)\n",
    "    y = pm.Normal(\"y\", x, 1, observed=data)\n",
    "\n",
    "dataloader = pm.Dataloader(np.random.normal(10_000, 2), batch_size=64)\n",
    "\n",
    "with model:\n",
    "    trainer = Trainer(method=ADVI(), dataloader=dataloader)\n",
    "    trainer.fit(n=10_000)\n",
    "````\n",
    "\n",
    "Importantly, the model doesn't need to know about the dataloader. We will need to tweak the inference object, but it's not so bad.\n",
    "\n",
    "````python\n",
    "class ADVI(Inference):\n",
    "    def step(self, batch):\n",
    "        self.model.set_data(\"data\", batch)\n",
    "        ...\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ba769-fb8f-47a7-82b6-ab6ca13ad61e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc-dev",
   "language": "python",
   "name": "pymc-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
