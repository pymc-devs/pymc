{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Factors and Marginal Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on PyMC3 v3.4.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import betaln\n",
    "from scipy.stats import beta\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Bayesian way\" to compare models is to compute the _marginal likelihood_ of each model $p(y \\mid M_k)$, _i.e._ the probability of the observed data $y$ given the $M_k$ model. This quantity, the marginal likelihood, is just the normalizing constant of Bayes' theorem. We can see this if we write Bayes' theorem and make explicit the fact that all inferences are model-dependant. \n",
    "\n",
    "$$p (\\theta \\mid y, M_k ) = \\frac{p(y \\mid \\theta, M_k) p(\\theta \\mid M_k)}{p( y \\mid M_k)}$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $y$ is the data\n",
    "* $\\theta$ the parameters\n",
    "* $M_k$ one model out of K competing models\n",
    "\n",
    "\n",
    "Usually when doing inference we do not need to compute this normalizing constant, so in practice we often compute the posterior up to a constant factor, that is:\n",
    "\n",
    "$$p (\\theta \\mid y, M_k ) \\propto p(y \\mid \\theta, M_k) p(\\theta \\mid M_k)$$\n",
    "\n",
    "However, for model comparison and model averaging the marginal likelihood is an important quantity. Although, it's not the only way to perform these tasks, you can read about model averaging and model selection using alternative methods [here](model_comparison.ipynb), [there](model_averaging.ipynb) and [elsewhere](GLM-model-selection.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian model selection\n",
    "\n",
    "If our main objective is to choose only one model, the _best_ one, from a set of models we can just choose the one with the largest $p(y \\mid M_k)$. This is totally fine if all models are assumed to have the same _a priori_ probability. Otherwise, we have to take into account that not all models are equally likely _a priori_ and compute:\n",
    "\n",
    "$$p(M_k \\mid y) \\propto p(y \\mid M_k) p(M_k)$$\n",
    "\n",
    "Sometimes the main objective is not to just keep a single model but instead to compare models to determine which ones are more likely and by how much. This can be achieved using Bayes factors:\n",
    "\n",
    "$$BF =  \\frac{p(y \\mid M_0)}{p(y \\mid M_1)}$$\n",
    "\n",
    "that is, the ratio between the marginal likelihood of two models. The larger the BF the _better_ the model in the numerator ($M_0$ in this example). To ease the interpretation of BFs some authors have proposed tables with levels of _support_ or _strength_, just a way to put numbers into words. \n",
    "\n",
    "* 1-3: anecdotal\n",
    "* 3-10: moderate\n",
    "* 10-30: strong\n",
    "* 30-100: very strong\n",
    "* $>$ 100: extreme\n",
    "\n",
    "Notice that if you get numbers below 1 then the support is for the model in the denominator, tables for those cases are also available. Of course, you can also just take the inverse of the values in the above table or take the inverse of the BF value and you will be OK.\n",
    "\n",
    "Is very important to remember that these rules are just conventions, simple guides at best. Results should always be put into context of our problems and should be accompanied with enough details so others could evaluate by themselves if they agree with our conclusions. The evidence necessary to make a claim is not the same in particle physics, or a court, or to evacuate a town to prevent hundreds of deaths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian model averaging\n",
    "\n",
    "Instead of choosing one single model from a set of candidate models, model averaging is about getting one meta-model by averaging the separate models. The Bayesian version of this weights each model by its marginal posterior probability.\n",
    "\n",
    "$$p(\\theta \\mid y) = \\sum_{k=1}^K p(\\theta \\mid y, M_k) p(M_k \\mid y)$$\n",
    "\n",
    "This is the optimal way to average models if the prior is _correct_ and the _correct_ model is one of the $M_k$ models in our set. Otherwise, _bayesian model averaging_ will asymptotically select the one single model in the set of compared models that is closest in [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n",
    "\n",
    "Check this [example](model_averaging.ipynb) as an alternative way to perform model averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Some remarks\n",
    "\n",
    "Now we will briefly discuss some key facts about the _marginal likelihood_\n",
    "\n",
    "* The good\n",
    "    * **Occam Razor included**: Models with more parameters have a larger penalization than models with fewer parameters. The intuitive reason is that the larger the number of parameters the more _spread_ the _prior_ with respect to the likelihood.\n",
    "\n",
    "\n",
    "* The bad\n",
    "    * Computing the marginal likelihood is, generally, a hard task because it’s an integral of a highly variable function over a high dimensional parameter space. In general this integral needs to be solved numerically using more or less sophisticated methods.\n",
    "    \n",
    "$$p(y \\mid M_k) = \\int_{\\theta_k} p(y \\mid \\theta_k, M_k) p(\\theta_k, M_k) d\\theta_k$$\n",
    "\n",
    "* The ugly\n",
    "    * The marginal likelihood depends **sensitively** on the specified prior $p(\\theta_k \\mid M_k)$ for each model.\n",
    "\n",
    "Notice that _the good_ and _the ugly_ are related. Using the marginal likelihood to compare models is a good idea because a penalization for complex models is already included (thus preventing us from overfitting) and, at the same time, a change in the prior will affect the computations of the _marginal likelihood_. At first this sounds a little bit silly we already know that priors affect computations (otherwise we could simply avoid them), but the point here is the word **sensitively**. We are talking about changes in the prior that will keep inference of $\\theta$ more or less the same, but could have a big impact in the value of the marginal likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Bayes factors\n",
    "\n",
    "The _marginal likelihood_ is generally not available in closed-form except for some restricted models. For this reason many methods have been devised to compute the _marginal likelihood_ and the derived Bayes factors, some of these methods are so simple and [naive](https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever/) that works very bad in practice. Most of the useful methods have been originally proposed in the field of Statistical Mechanics. This connection is explained because the marginal likelihood is analogous to a central quantity in statistical physics known as the _partition function_ which in turn is closely related to another very important quantity the _free-energy_. Many of the connections between Statistical Mechanics and Bayesian inference are summarized [here](https://arxiv.org/abs/1706.01428)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a hierarchical model\n",
    "\n",
    "Computation of Bayes factors can be framed as a hierarchical model, where the high-level parameter is an index assigned to each model and sampled from a categorical distribution. In other words, we perform inference for two (or more) competing models at the same time and we use a discrete _dummy_ variable that _jumps_ between models. How much time we spend sampling each model is proportional to $p(M_k \\mid y)$.\n",
    "\n",
    "Some common problems when computing Bayes factors this way is that if one model is better than the other, by definition, we will spend more time sampling from it than from the other model. And this could lead to inaccuracies because we will be undersampling the less likely model. Another problem is that the values of the parameters get updated even when the parameters are not used to fit that model. That is, when model 0 is chosen, parameters in model 1 are updated but since they are not used to explain the data, they only get restricted by the prior. If the prior is too vague, it is possible that when we choose model 1, the parameter values are too far away from the previous accepted values and hence the step is rejected. Therefore we end up having a problem with sampling.\n",
    "\n",
    "In case we find these problems, we can try to improve sampling by implementing two modifications to our model:\n",
    "\n",
    "* Ideally, we can get a better sampling of both models if they are visited equally, so we can adjust the prior for each model in such a way to favour the less favourable model and disfavour the most favourable one. This will not affect the computation of the Bayes factor because we have to include the priors in the computation.\n",
    "\n",
    "* Use pseudo priors, as suggested by Kruschke and others. The idea is simple: if the problem is that the parameters drift away unrestricted, when the model they belong to is not selected, then one solution is to try to restrict them artificially, but only when not used! You can find an example of using pseudo priors in a model used by Kruschke in his book and [ported](https://github.com/aloctavodia/Doing_bayesian_ data_analysis.) to Python/PyMC3.\n",
    "\n",
    "If you want to learn more about this approach to the computation of the marginal likelihood see [Chapter 12 of Doing Bayesian Data Analysis](http://www.sciencedirect.com/science/book/9780124058880). This chapter also discuss how to use Bayes Factors as a Bayesian alternative to classical hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytically\n",
    "\n",
    "For some models, like the beta-binomial model (AKA the _coin-flipping_ model) we can compute the marginal likelihood analytically. If we write this model as:\n",
    "\n",
    "$$\\theta \\sim Beta(\\alpha, \\beta)$$\n",
    "$$y \\sim Bin(n=1, p=\\theta)$$\n",
    "\n",
    "the _marginal likelihood_ will be:\n",
    "\n",
    "$$p(y) = \\binom {n}{h}  \\frac{B(\\alpha + h,\\ \\beta + n - h)} {B(\\alpha, \\beta)}$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $B$ is the [beta function](https://en.wikipedia.org/wiki/Beta_function) not to get confused with the $Beta$ distribution\n",
    "* $n$ is the number of trials\n",
    "* $h$ is the number of success\n",
    "\n",
    "Since we only care about the relative value of the _marginal likelihood_ under two different models (for the same data), we can omit the binomial coefficient $\\binom {n}{h}$, thus we can write:\n",
    "\n",
    "$$p(y) \\propto \\frac{B(\\alpha + h,\\ \\beta + n - h)} {B(\\alpha, \\beta)}$$\n",
    "\n",
    "This have been coded in the following cell, with a twist we will be using the `betaln` function instead of the `beta` function, this is done to prevent underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_binom(prior, y):\n",
    "    \"\"\"\n",
    "    Compute the marginal likelihood, analytically, for a beta-binomial model.\n",
    "\n",
    "    prior : tuple\n",
    "        tuple of alpha and beta parameter for the prior (beta distribution)\n",
    "    y : array\n",
    "        array with \"1\" and \"0\" corresponding to the success and fails respectively\n",
    "    \"\"\"\n",
    "    alpha, beta = prior\n",
    "    h = np.sum(y)\n",
    "    n = len(y)\n",
    "    p_y = np.exp(betaln(alpha + h, beta+n-h) - betaln(alpha, beta))\n",
    "    return p_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data for this example consist on 100 \"flips of a coin\" and the same number of observed \"heads\" and \"tails\". We will compare two models one with a uniform prior and one with a _more concentrated_ prior around $\\theta = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.repeat([1, 0], [50, 50])  # 50 \"heads\" and 50 \"tails\"\n",
    "priors = ((1, 1), (30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for a, b in priors:\n",
    "    distri = beta(a, b)\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    x_pdf = distri.pdf(x)\n",
    "    plt.plot (x, x_pdf, label=r'$\\alpha$ = {:d}, $\\beta$ = {:d}'.format(a, b))\n",
    "    plt.yticks([])\n",
    "    plt.xlabel('$\\\\theta$')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell returns the Bayes factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "BF = (beta_binom(priors[1], y) / beta_binom(priors[0], y))\n",
    "print(round(BF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model with the more concentrated prior $Beta(30, 30)$ has $\\approx 5$ times more support than the model with the more extended prior $Beta(1, 1)$. Besides the exact numerical value this should not be surprising since the prior for the most favoured model is concentrated around $\\theta = 0.5$ and the data $y$ has equal number of head and tails, consintent with a value of $\\theta$ around 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes factors and inference\n",
    "\n",
    "In this example we have used Bayes factors to judge which model seems to be better at explaining the data, and we get that one of the models is $\\approx 5$ _better_ than the other. \n",
    "\n",
    "But what about the posterior we get from these models? How different they are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'traces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8399e40a30fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvarnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'traces' is not defined"
     ]
    }
   ],
   "source": [
    "pm.summary(traces[0], varnames='a').round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(traces[1], varnames='a').round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may argue that the results are pretty similar, we have the same mean value for $\\theta$, and a slightly wider posterior for `model_0`, as expected since this model has a wider prior. We can also check the posterior predictive distribution to see how similar they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ppc_0 = pm.sample_posterior_predictive(traces[0], 1000, models[0], size=(len(y), 20))\n",
    "ppc_1 = pm.sample_posterior_predictive(traces[1], 1000, models[1], size=(len(y), 20))\n",
    "for m_0, m_1 in zip(ppc_0['yl'].T, ppc_1['yl'].T):\n",
    "    pm.kdeplot(np.mean(m_0, 0), ax=ax, color='C0', label='model 0')\n",
    "    pm.kdeplot(np.mean(m_1, 0), ax=ax, color='C1', label='model 1')\n",
    "plt.xlabel(u'θ')  # matplotlib might have issues with displaying 'theta' string\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing, while is true that the observed data $y$ is more consistent with `model_1` than `model_0`, the posteriors distributions we got from both models are similar and hence if we use these posteriors to make predictions we will get similar results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
